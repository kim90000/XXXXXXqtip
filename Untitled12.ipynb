{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4a67260538ed478d889b55c935606e14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8cef9c21e9ae4c7b8ca9d922b08584af",
              "IPY_MODEL_d128928506b74d2b801c832cba858a78",
              "IPY_MODEL_3a41c60d226e4a0eab69e8d17c85da2e"
            ],
            "layout": "IPY_MODEL_8a44155379d94bb7b9e9973dc5c4ca65"
          }
        },
        "8cef9c21e9ae4c7b8ca9d922b08584af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c609ede361e455087abe4ac9dd920bf",
            "placeholder": "​",
            "style": "IPY_MODEL_1d14c591ff7e4a77b41b63e38fa11d56",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "d128928506b74d2b801c832cba858a78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8bbe38b75874091864fbab9a08b4050",
            "max": 50500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da4a0388294e4c70a25819e6302f9cc9",
            "value": 50500
          }
        },
        "3a41c60d226e4a0eab69e8d17c85da2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8410b90e380e49bf96b0acb053a94076",
            "placeholder": "​",
            "style": "IPY_MODEL_db988e5066c9427c93049476bf6f9296",
            "value": " 50.5k/50.5k [00:00&lt;00:00, 4.68MB/s]"
          }
        },
        "8a44155379d94bb7b9e9973dc5c4ca65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c609ede361e455087abe4ac9dd920bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d14c591ff7e4a77b41b63e38fa11d56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8bbe38b75874091864fbab9a08b4050": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da4a0388294e4c70a25819e6302f9cc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8410b90e380e49bf96b0acb053a94076": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db988e5066c9427c93049476bf6f9296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f6356e237f14f72abeaf44b14221bad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b6e2e4de1b934d64adc3aad666640b01",
              "IPY_MODEL_efa244a934cd4c28a82eebc186503bf7",
              "IPY_MODEL_f1902974934244c59f1c6c6d6fd2a9d7"
            ],
            "layout": "IPY_MODEL_a9e2cc5a6d6a4973b57b18e10614fc9c"
          }
        },
        "b6e2e4de1b934d64adc3aad666640b01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57a211df26474a29b0406c5b2aec8b77",
            "placeholder": "​",
            "style": "IPY_MODEL_0f7e089da39944ebabbc6bb999e89d1d",
            "value": "tokenizer.json: 100%"
          }
        },
        "efa244a934cd4c28a82eebc186503bf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0950cb7877744cb81326c73c2225f2e",
            "max": 9085657,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_907e8de6739c411c800dd4d277b66237",
            "value": 9085657
          }
        },
        "f1902974934244c59f1c6c6d6fd2a9d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1c8f1e74f274c1697ac67b4a43486ce",
            "placeholder": "​",
            "style": "IPY_MODEL_b68308d14bdf491d8044a27ffbfce322",
            "value": " 9.09M/9.09M [00:00&lt;00:00, 18.0MB/s]"
          }
        },
        "a9e2cc5a6d6a4973b57b18e10614fc9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57a211df26474a29b0406c5b2aec8b77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f7e089da39944ebabbc6bb999e89d1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0950cb7877744cb81326c73c2225f2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "907e8de6739c411c800dd4d277b66237": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1c8f1e74f274c1697ac67b4a43486ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b68308d14bdf491d8044a27ffbfce322": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e97f5314a8fb4fc5b158ff5f55b35e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90fb1aa45ddb41658d37453082b93bb4",
              "IPY_MODEL_15a0033542c847ba9f427d206fbfe457",
              "IPY_MODEL_98f4ed70ed9d414b89e6bac6a80bfc6a"
            ],
            "layout": "IPY_MODEL_9f20ffd82c934c44a73d3fe1d73a7d21"
          }
        },
        "90fb1aa45ddb41658d37453082b93bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_115528dec6a146fab11dae9f975f6299",
            "placeholder": "​",
            "style": "IPY_MODEL_cbbd31fa2b544015a54d16f9b4619773",
            "value": "config.json: 100%"
          }
        },
        "15a0033542c847ba9f427d206fbfe457": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43aea904d5ea430288f01d1dda42e93d",
            "max": 1117,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36855321a79c439aa950f7c35d19ae8e",
            "value": 1117
          }
        },
        "98f4ed70ed9d414b89e6bac6a80bfc6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71a29153ebff4775afa5966959f3d673",
            "placeholder": "​",
            "style": "IPY_MODEL_a0ea013b9fd54853ad76c6b0144449b0",
            "value": " 1.12k/1.12k [00:00&lt;00:00, 68.0kB/s]"
          }
        },
        "9f20ffd82c934c44a73d3fe1d73a7d21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "115528dec6a146fab11dae9f975f6299": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbbd31fa2b544015a54d16f9b4619773": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43aea904d5ea430288f01d1dda42e93d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36855321a79c439aa950f7c35d19ae8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71a29153ebff4775afa5966959f3d673": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0ea013b9fd54853ad76c6b0144449b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bdf0c80a75cf4753a678780935f087e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_524a8931b4074ed196687117f4dc5cbe",
              "IPY_MODEL_0d2ed0b0c9f0483dad36a867c2f15396",
              "IPY_MODEL_4d0d65f0535c4c1881a63040f2530e64"
            ],
            "layout": "IPY_MODEL_bcc79f8c6ce049419d62038d6e96a1e7"
          }
        },
        "524a8931b4074ed196687117f4dc5cbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb1e48a38393422f8b15504937dc70d7",
            "placeholder": "​",
            "style": "IPY_MODEL_53131e9d2a7e4526984bdfd27da09cfc",
            "value": "model.safetensors: 100%"
          }
        },
        "0d2ed0b0c9f0483dad36a867c2f15396": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8af7b822097c4d7191927344f3b890b8",
            "max": 3852517272,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c64e639577cc4a7fb9def4026900d1a1",
            "value": 3852517272
          }
        },
        "4d0d65f0535c4c1881a63040f2530e64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bda83ee17df4a9eb67c0d5249951b0a",
            "placeholder": "​",
            "style": "IPY_MODEL_9fdd6074307b4eafac304cbf6cb4d446",
            "value": " 3.85G/3.85G [00:47&lt;00:00, 81.2MB/s]"
          }
        },
        "bcc79f8c6ce049419d62038d6e96a1e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb1e48a38393422f8b15504937dc70d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53131e9d2a7e4526984bdfd27da09cfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8af7b822097c4d7191927344f3b890b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c64e639577cc4a7fb9def4026900d1a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5bda83ee17df4a9eb67c0d5249951b0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fdd6074307b4eafac304cbf6cb4d446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a64eab1f5d844025829fd73d3b4ef713": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_237775f85157454d8a67e1d8b52debad",
              "IPY_MODEL_4bc2e34107854bf7b810a97c284605a9",
              "IPY_MODEL_4aa01775d4054302968daad5c2b1ee6a"
            ],
            "layout": "IPY_MODEL_e6ea9f05581f47779c0d45a2fe50a63e"
          }
        },
        "237775f85157454d8a67e1d8b52debad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aad9af6d1c8d41719d6a6897156cd384",
            "placeholder": "​",
            "style": "IPY_MODEL_3ae35104cef64397bb799ccd96865ecd",
            "value": "config.json: 100%"
          }
        },
        "4bc2e34107854bf7b810a97c284605a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_839b20dddac249e8a6adfaee40ed0bc0",
            "max": 912,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e050ef8144e4921921a8c36f5e83bf7",
            "value": 912
          }
        },
        "4aa01775d4054302968daad5c2b1ee6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d9f01cff54b493abdeebf5ff429e624",
            "placeholder": "​",
            "style": "IPY_MODEL_60f7e36bd24f4bddb7ed95590db1a6d5",
            "value": " 912/912 [00:00&lt;00:00, 20.7kB/s]"
          }
        },
        "e6ea9f05581f47779c0d45a2fe50a63e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aad9af6d1c8d41719d6a6897156cd384": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ae35104cef64397bb799ccd96865ecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "839b20dddac249e8a6adfaee40ed0bc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e050ef8144e4921921a8c36f5e83bf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d9f01cff54b493abdeebf5ff429e624": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60f7e36bd24f4bddb7ed95590db1a6d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtOtNaN4mqRV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UgfGVK5imuY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2r92xzqRmubx",
        "outputId": "aa400718-c146-432a-b51c-7cde02ec8092"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.45.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import os\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# Check if CUDA is available\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "\n",
        "# Load the tokenizer\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "print(\"Tokenizer loaded successfully!\")\n",
        "\n",
        "# Load the model without additional quantization since it's already quantized to 2-bit\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Function to generate responses\n",
        "def generate_response(prompt, max_new_tokens=512):\n",
        "    # Format prompt for Llama 3.1\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    # Format input for model\n",
        "    input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate response\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "\n",
        "    # Decode response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract assistant's response (remove the prompt)\n",
        "    assistant_response = response[len(input_text):]\n",
        "\n",
        "    return assistant_response\n",
        "\n",
        "# Example usage\n",
        "prompt = \"Explain quantum computing in simple terms.\"\n",
        "print(\"\\nGenerating response to:\", prompt)\n",
        "response = generate_response(prompt)\n",
        "print(\"\\nModel response:\", response)\n",
        "\n",
        "# Interactive mode\n",
        "def interactive_chat():\n",
        "    print(\"\\n=== Interactive Chat Mode ===\")\n",
        "    print(\"Type 'exit' to end the conversation.\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Ending chat session.\")\n",
        "            break\n",
        "\n",
        "        print(\"\\nGenerating response...\")\n",
        "        response = generate_response(user_input)\n",
        "        print(f\"\\nAssistant: {response}\")\n",
        "\n",
        "# Start interactive chat\n",
        "if __name__ == \"__main__\":\n",
        "    interactive_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356,
          "referenced_widgets": [
            "4a67260538ed478d889b55c935606e14",
            "8cef9c21e9ae4c7b8ca9d922b08584af",
            "d128928506b74d2b801c832cba858a78",
            "3a41c60d226e4a0eab69e8d17c85da2e",
            "8a44155379d94bb7b9e9973dc5c4ca65",
            "0c609ede361e455087abe4ac9dd920bf",
            "1d14c591ff7e4a77b41b63e38fa11d56",
            "d8bbe38b75874091864fbab9a08b4050",
            "da4a0388294e4c70a25819e6302f9cc9",
            "8410b90e380e49bf96b0acb053a94076",
            "db988e5066c9427c93049476bf6f9296",
            "8f6356e237f14f72abeaf44b14221bad",
            "b6e2e4de1b934d64adc3aad666640b01",
            "efa244a934cd4c28a82eebc186503bf7",
            "f1902974934244c59f1c6c6d6fd2a9d7",
            "a9e2cc5a6d6a4973b57b18e10614fc9c",
            "57a211df26474a29b0406c5b2aec8b77",
            "0f7e089da39944ebabbc6bb999e89d1d",
            "a0950cb7877744cb81326c73c2225f2e",
            "907e8de6739c411c800dd4d277b66237",
            "c1c8f1e74f274c1697ac67b4a43486ce",
            "b68308d14bdf491d8044a27ffbfce322",
            "e97f5314a8fb4fc5b158ff5f55b35e44",
            "90fb1aa45ddb41658d37453082b93bb4",
            "15a0033542c847ba9f427d206fbfe457",
            "98f4ed70ed9d414b89e6bac6a80bfc6a",
            "9f20ffd82c934c44a73d3fe1d73a7d21",
            "115528dec6a146fab11dae9f975f6299",
            "cbbd31fa2b544015a54d16f9b4619773",
            "43aea904d5ea430288f01d1dda42e93d",
            "36855321a79c439aa950f7c35d19ae8e",
            "71a29153ebff4775afa5966959f3d673",
            "a0ea013b9fd54853ad76c6b0144449b0",
            "bdf0c80a75cf4753a678780935f087e2",
            "524a8931b4074ed196687117f4dc5cbe",
            "0d2ed0b0c9f0483dad36a867c2f15396",
            "4d0d65f0535c4c1881a63040f2530e64",
            "bcc79f8c6ce049419d62038d6e96a1e7",
            "cb1e48a38393422f8b15504937dc70d7",
            "53131e9d2a7e4526984bdfd27da09cfc",
            "8af7b822097c4d7191927344f3b890b8",
            "c64e639577cc4a7fb9def4026900d1a1",
            "5bda83ee17df4a9eb67c0d5249951b0a",
            "9fdd6074307b4eafac304cbf6cb4d446"
          ]
        },
        "id": "fSxwm-_bmueZ",
        "outputId": "da7bb004-85f5-4983-fbf3-7a36d2441169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n",
            "Loading tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a67260538ed478d889b55c935606e14"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f6356e237f14f72abeaf44b14221bad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer loaded successfully!\n",
            "Loading model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e97f5314a8fb4fc5b158ff5f55b35e44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.85G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bdf0c80a75cf4753a678780935f087e2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#include <cstdio>\n",
        "#include <cassert>\n",
        "#include <climits>\n",
        "\n",
        "#include <cstdlib>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cuda/pipeline>\n",
        "#include <cuda_fp16.h>\n",
        "#include <mma.h>\n",
        "#include <c10/cuda/CUDAStream.h>\n",
        "\n",
        "#include \"inference.h\"\n",
        "\n",
        "using namespace nvcuda;\n",
        "\n",
        "\n",
        "#define CHECK_CUDA(x)           TORCH_CHECK(x.is_cuda(), #x \" must be a CUDA tensor\")\n",
        "#define CHECK_CONTIGUOUS(x)     TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_INPUT(x)          do { CHECK_CUDA(x); CHECK_CONTIGUOUS(x); } while (false)\n",
        "\n",
        "#define BLOCKS_PER_SM 1\n",
        "#define MMA_M                   16\n",
        "#define MMA_N                   8\n",
        "#define MMA_K                   16\n",
        "\n",
        "#define BLOCK_COUNT             128\n",
        "//#define MAX_THREADS_PER_SM      2048\n",
        "#define WARP_SIZE               32\n",
        "#define BLOCK_SIZE              1024\n",
        "#define WARPS_PER_BLOCK         (BLOCK_SIZE/WARP_SIZE)\n",
        "\n",
        "#define PREFETCHW               4\n",
        "#define PREFETCHX               4\n",
        "#define BLOCKS_PER_SM           1\n",
        "\n",
        "#define FULL_MASK               0xFFFFFFFFU\n",
        "\n",
        "__inline__ __device__ uint32_t ld_cs(const uint32_t* p)\n",
        "{\n",
        "    uint32_t out;\n",
        "    asm(\"ld.global.cs.u32 %0, [%1];\" : \"=r\"(out) : \"l\"(p));\n",
        "    return out;\n",
        "}\n",
        "\n",
        "__inline__ __device__ uint2 ld_cs(const uint2* p)\n",
        "{\n",
        "    uint2 out;\n",
        "    asm(\"ld.global.cs.v2.u32 {%0, %1}, [%2];\" : \"=r\"(out.x), \"=r\"(out.y) : \"l\"(p));\n",
        "    //asm(\"ld.weak.global.cs.L2::256B.v2.u32 {%0, %1}, [%2];\" : \"=r\"(out.x), \"=r\"(out.y) : \"l\"(p));\n",
        "    // the compiler doesn't know how to infer load(p) and load(p+4096) from loop unrolling with this :(\n",
        "    return out;\n",
        "}\n",
        "__inline__ __device__ uint3 ld_cs(const uint3* p)\n",
        "{\n",
        "    uint3 out;\n",
        "    asm(\"ld.global.cs.u32 %0, [%1];\" : \"=r\"(out.x) : \"l\"(p));\n",
        "    asm(\"ld.global.cs.u32 %0, [%1+4];\" : \"=r\"(out.y) : \"l\"(p));\n",
        "    asm(\"ld.global.cs.u32 %0, [%1+8];\" : \"=r\"(out.z) : \"l\"(p));\n",
        "    return out;\n",
        "}\n",
        "__inline__ __device__ uint4 ld_cs(const uint4* p)\n",
        "{\n",
        "    uint4 out;\n",
        "    asm(\"ld.global.cs.v4.u32 {%0, %1, %2, %3}, [%4];\" : \"=r\"(out.x), \"=r\"(out.y), \"=r\"(out.z), \"=r\"(out.w) : \"l\"(p));\n",
        "    return out;\n",
        "}\n",
        "__inline__ __device__ uint2 ld_x(const uint32_t* p, uint32_t x_idx, int subki)\n",
        "{\n",
        "    uint2 out;\n",
        "    // the indexing is written as int32 math instead of lsu constant offset because\n",
        "    // apparently using lsu offset adds lots of MIO pressure!\n",
        "    if (subki == 0) {\n",
        "        asm(\"ld.global.L1::evict_last.u32 %0, [%1];\" : \"=r\"(out.x) : \"l\"(p+x_idx));\n",
        "        asm(\"ld.global.L1::evict_last.u32 %0, [%1];\" : \"=r\"(out.y) : \"l\"(p+(x_idx+4)));\n",
        "    } else {\n",
        "        asm(\"ld.global.L1::evict_last.u32 %0, [%1];\" : \"=r\"(out.x) : \"l\"(p+(x_idx+8)));\n",
        "        asm(\"ld.global.L1::evict_last.u32 %0, [%1];\" : \"=r\"(out.y) : \"l\"(p+(x_idx+12)));\n",
        "    }\n",
        "    return out;\n",
        "}\n",
        "__inline__ __device__ uint32_t ld_x(const uint32_t* p)\n",
        "{\n",
        "    uint32_t out;\n",
        "    asm(\"ld.global.L1::evict_last.u32 %0, [%1];\" : \"=r\"(out) : \"l\"(p));\n",
        "    return out;\n",
        "}\n",
        "\n",
        "__inline__ __device__ void prefetch(uint32_t *a){\n",
        "    asm(\"prefetch.global.L1 [%0];\"::\"l\"(a));\n",
        "}\n",
        "\n",
        "#define LD_CS\n",
        "template <uint32_t R>\n",
        "__device__ inline void load_reg_cs(const uint16_t *__restrict__ compressed, int weight_idx, uint32_t laneId, uint4 &reg_cs_next, uint4 &reg_cs2_next) {\n",
        "    if constexpr(R == 2) {\n",
        "#ifdef LD_CS\n",
        "        ditto2 reg_load = {.u32x2 = ld_cs((uint2 *) &compressed[weight_idx])};\n",
        "#else\n",
        "        ditto2 reg_load = {.u16x4 = *((ushort4 * )(compressed + weight_idx))};\n",
        "#endif\n",
        "        uint32_t next1 = __shfl_sync(FULL_MASK, reg_load.u32x2.x, laneId + 1);\n",
        "        uint32_t next2 = __shfl_sync(FULL_MASK, reg_load.u32x2.y, laneId + 1);\n",
        "        reg_cs_next.x = __byte_perm(next1, reg_load.u32x2.x, 0x5410);\n",
        "        reg_cs_next.y = __byte_perm(next1, reg_load.u32x2.x, 0x7632);\n",
        "        reg_cs_next.z = __byte_perm(next2, reg_load.u32x2.y, 0x5410);\n",
        "        reg_cs_next.w = __byte_perm(next2, reg_load.u32x2.y, 0x7632);\n",
        "    } else if constexpr(R == 3) {\n",
        "#ifdef LD_CS\n",
        "        uint3 reg_load = ld_cs((uint3 *) &compressed[weight_idx]);\n",
        "        uint32_t reg_load1 = reg_load.x, reg_load2 = reg_load.y, reg_load3 = reg_load.z;\n",
        "#else\n",
        "        uint32_t reg_load1 = *((uint32_t *) &compressed[weight_idx]);\n",
        "        uint32_t reg_load2 = *((uint32_t *) &compressed[weight_idx + 2]);\n",
        "        uint32_t reg_load3 = *((uint32_t *) &compressed[weight_idx + 4]);\n",
        "#endif\n",
        "\n",
        "        uint32_t reg_24_1 = reg_load1 & 0xffffff;\n",
        "        uint32_t reg_24_2 = ((reg_load1 >> 24) | (reg_load2 << 8)) & 0xffffff;\n",
        "        uint32_t reg_24_3 = ((reg_load2 >> 16) | (reg_load3 << 16)) & 0xffffff;\n",
        "        uint32_t reg_24_4 = (reg_load3 >> 8) & 0xffffff;\n",
        "\n",
        "        // send high 16 bits to prev thread\n",
        "        uint32_t pack1 = (reg_24_1 >> 8) | ((reg_24_2 << 8) & 0xffff0000);\n",
        "        uint32_t pack3 = (reg_24_3 >> 8) | ((reg_24_4 << 8) & 0xffff0000);\n",
        "\n",
        "        // receive high 16 bits from next thread\n",
        "        uint32_t next1 = __shfl_sync(FULL_MASK, pack1, laneId + 1);\n",
        "        uint32_t next3 = __shfl_sync(FULL_MASK, pack3, laneId + 1);\n",
        "\n",
        "        reg_cs_next.x = __byte_perm(next1, reg_24_1, 0x6541);\n",
        "        reg_cs_next.y = __byte_perm(next1, reg_24_2, 0x6543);\n",
        "        reg_cs_next.z = __byte_perm(next3, reg_24_3, 0x6541);\n",
        "        reg_cs_next.w = __byte_perm(next3, reg_24_4, 0x6543);\n",
        "\n",
        "        reg_cs2_next.x = ((next1 >> 6) & 0b11'1111'1111) | (reg_24_1 << 10);\n",
        "        reg_cs2_next.y = ((next1 >> (6 + 16) & 0b11'1111'1111)) | (reg_24_2 << 10);\n",
        "        reg_cs2_next.z = ((next3 >> 6) & 0b11'1111'1111) | (reg_24_3 << 10);\n",
        "        reg_cs2_next.w = ((next3 >> (6 + 16) & 0b11'1111'1111)) | (reg_24_4 << 10);\n",
        "    } else if constexpr(R == 4) {\n",
        "#ifdef LD_CS\n",
        "        uint4 reg_load = ld_cs((uint4 *) &compressed[weight_idx]);\n",
        "#else\n",
        "        uint4 reg_load = *((uint4 *) &compressed[weight_idx]);\n",
        "#endif\n",
        "        uint32_t reg_load1 = reg_load.x, reg_load2 = reg_load.y, reg_load3 = reg_load.z, reg_load4 = reg_load.w;\n",
        "\n",
        "        // send high 16 bits to prev thread\n",
        "        uint32_t pack1 = (reg_load1 >> 16) | (reg_load2 & 0xffff0000);\n",
        "        uint32_t pack3 = (reg_load3 >> 16) | (reg_load4 & 0xffff0000);\n",
        "\n",
        "        uint32_t next1 = __shfl_sync(FULL_MASK, pack1, laneId + 1);\n",
        "        uint32_t next3 = __shfl_sync(FULL_MASK, pack3, laneId + 1);\n",
        "\n",
        "        reg_cs_next.x = reg_load1;\n",
        "        reg_cs_next.y = reg_load2;\n",
        "        reg_cs_next.z = reg_load3;\n",
        "        reg_cs_next.w = reg_load4;\n",
        "\n",
        "        reg_cs2_next.x = __byte_perm(next1, reg_load1, 0x0041);\n",
        "        reg_cs2_next.y = __byte_perm(next1, reg_load2, 0x0043);\n",
        "        reg_cs2_next.z = __byte_perm(next3, reg_load3, 0x0041);\n",
        "        reg_cs2_next.w = __byte_perm(next3, reg_load4, 0x0043);\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "template <uint32_t L, uint32_t S, uint32_t R, uint32_t V, uint32_t M, uint32_t N, uint32_t K>\n",
        "__global__ static void\n",
        "__launch_bounds__(BLOCK_SIZE, 1)\n",
        "kernel_decompress_matvec(\n",
        "    float *__restrict__ out,\n",
        "    const uint32_t *__restrict__ compressed,\n",
        "    const half2 *__restrict__ x,\n",
        "    const half2 *__restrict__ codebook\n",
        ") {\n",
        "        // ** load codebook **\n",
        "    extern __shared__ __align__(1<<(5+V+1)) half2 smem_codebook[];\n",
        "\n",
        "    // ** cursed indexing math **\n",
        "\n",
        "    uint32_t threadId = threadIdx.x;\n",
        "    uint32_t laneId = threadIdx.x % WARP_SIZE;\n",
        "    uint32_t warpId = threadId / WARP_SIZE;\n",
        "    uint32_t blockId = blockIdx.x;\n",
        "\n",
        "    constexpr uint32_t tileCountM = M / MMA_M;\n",
        "    constexpr uint32_t tileCountK = K / MMA_K;\n",
        "\n",
        "    constexpr uint32_t warps_per_block = BLOCK_SIZE / WARP_SIZE;\n",
        "\n",
        "#define ROUND_UP(a, b) ((a + b - 1) / b)\n",
        "\n",
        "    static_assert (tileCountM % 2 == 0);\n",
        "    constexpr uint32_t m_per_block = ROUND_UP(tileCountM, (2 * BLOCK_COUNT));\n",
        "    // tiles are iterated along k in groups of 2\n",
        "    //static_assert (tileCountK >= warps_per_block * 2);\n",
        "    constexpr uint32_t k_per_block = tileCountK / (warps_per_block * 4) * 2;\n",
        "    // we sync at ki%4==0, make sure this is safe\n",
        "    //constexpr bool enable_kim4_sync = !(M == 4096 && K==4096) && (tileCountK % (warps_per_block * 2) == 0 || k_per_block % 4 != 0);\n",
        "    // some warps have more k tiles\n",
        "    static_assert((tileCountK % (warps_per_block * 4)) % 4 == 0);\n",
        "    uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
        "\n",
        "    constexpr uint32_t u16_per_compressed_tile = MMA_M * MMA_K * R / 16;\n",
        "    static_assert((MMA_M * MMA_K * R) % 16 == 0);\n",
        "    constexpr uint32_t f16x2_per_x_tile = MMA_K / 2;\n",
        "    constexpr uint32_t f32_per_out_tile = MMA_M;\n",
        "\n",
        "    uint32_t tileIdM = m_per_block * blockId;\n",
        "\n",
        "    constexpr uint32_t weight_block = 4;\n",
        "    constexpr uint32_t u16_per_tile_block = u16_per_compressed_tile * weight_block; // one tile block per warp at a time\n",
        "    constexpr uint32_t weight_step = warps_per_block * u16_per_tile_block;\n",
        "    constexpr uint32_t weight_row_step = tileCountK * u16_per_compressed_tile * 2;  // 2 rows of tiles\n",
        "\n",
        "\n",
        "\n",
        "    for (uint32_t mi = 0; mi < m_per_block; mi+=1) {\n",
        "        if (tileIdM * 2 >= tileCountM) return;\n",
        "        // ** load weight, start loop **\n",
        "        int weight_idx = tileIdM * weight_row_step + warpId * u16_per_tile_block * 2 + laneId * (u16_per_tile_block / WARP_SIZE);\n",
        "        uint4 reg_cs_next = {};\n",
        "        uint4 reg_cs2_next = {};\n",
        "        load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx, laneId, reg_cs_next, reg_cs2_next);\n",
        "        uint4 reg_cs;\n",
        "        uint4 reg_cs2;\n",
        "\n",
        "        // define acc\n",
        "        float4 reg_p[2] = {};\n",
        "\n",
        "#define LOAD_X_BUFFERED\n",
        "#ifdef PERMUTE_K\n",
        "        uint32_t x_idx = warpId * f16x4_per_x_tile*2 + laneId;\n",
        "        uint32_t x_idx_step = warps_per_block * f16x4_per_x_tile * 2;\n",
        "#else\n",
        "#if !defined(LOAD_X_SHUFFLE) && !defined(LOAD_X_BUFFERED)\n",
        "        uint32_t x_idx = warpId * f16x2_per_x_tile * 2 + laneId;  // every warp does 2 k tiles per iteration\n",
        "        uint32_t x_idx_step = warps_per_block * f16x2_per_x_tile * 2;\n",
        "#else\n",
        "        uint32_t x_idx = warpId * f16x2_per_x_tile * 4 + laneId;  // every warp does 4 k tiles per iteration\n",
        "        uint32_t x_idx_step = warps_per_block * f16x2_per_x_tile * 4;\n",
        "#endif\n",
        "#endif\n",
        "        if (mi == 0) {\n",
        "#define DO_LOAD_CODEBOOK\n",
        "#ifdef DO_LOAD_CODEBOOK\n",
        "            uint32_t my_cb_idx = threadIdx.x & 0x1ff;\n",
        "            half2 my_codebook_element = codebook[my_cb_idx];\n",
        "            for (uint32_t i = 0; i < 32; i+= 2) {\n",
        "                smem_codebook[(my_cb_idx << 5)|(i ^ (threadIdx.x & 0x1f) ^ (threadIdx.x >> 9))] = my_codebook_element;\n",
        "            }\n",
        "            // for (uint32_t i = 0; i < 32; i+= 1) { assert(smem_codebook[(my_cb_idx << 5) + i] == my_codebook_element); }\n",
        "            __syncthreads();\n",
        "#endif\n",
        "        }\n",
        "\n",
        "        __shared__ ditto2 x_buf[2][BLOCK_SIZE / WARP_SIZE][4][4];\n",
        "        uint32_t x_line;\n",
        "#pragma unroll 4\n",
        "        for (uint32_t ki = 0; ki < this_warp_k; ki += 1) {\n",
        "            // load this 2x2 block of weight tiles\n",
        "            if (ki + 1 != this_warp_k && ki % 2 == 1) weight_idx += weight_step * 2; // fixme: this costs 10GB/s\n",
        "            reg_cs = reg_cs_next;\n",
        "            reg_cs2 = reg_cs2_next;\n",
        "            load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx + (1 - ki % 2) * u16_per_tile_block, laneId, reg_cs_next, reg_cs2_next);\n",
        "\n",
        "#define LOAD_X\n",
        "#ifdef LOAD_X\n",
        "#ifdef LOAD_X_BUFFERED\n",
        "            if (ki % 2 == 0) {\n",
        "                __syncwarp();\n",
        "                x_buf[0][warpId][laneId / 8][laneId % 4].u32[(laneId % 8) / 4] = ld_x(reinterpret_cast<const uint32_t *>(x) + x_idx);\n",
        "                __syncwarp();\n",
        "                x_idx += x_idx_step;\n",
        "            }\n",
        "#else\n",
        "#ifdef LOAD_X_SHUFFLE\n",
        "            if (ki % 2 == 0) {\n",
        "                x_line = ld_x(((uint32_t *) x) + x_idx);\n",
        "                x_idx += x_idx_step;\n",
        "            }\n",
        "#endif\n",
        "#endif\n",
        "#endif\n",
        "\n",
        "#pragma unroll 2\n",
        "            for (uint32_t subki = 0; subki < 2; subki += 1) {\n",
        "                // load activations\n",
        "                // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#matrix-fragments-for-mma-m16n8k16-with-floating-point-type\n",
        "                ditto2 reg_a;\n",
        "#define LD_X\n",
        "#ifdef LOAD_X\n",
        "#ifdef LOAD_X_SHUFFLE\n",
        "                uint32_t x_subki = (ki % 2 * 2 + subki);\n",
        "                if (x_subki != 0) {\n",
        "                    reg_a.u32x2.x = __shfl_sync(FULL_MASK, x_line, (laneId & 3) | (8 * x_subki));\n",
        "                    reg_a.u32x2.y = __shfl_sync(FULL_MASK, x_line, (laneId & 3) | (4 | (8 * x_subki)));\n",
        "                } else {\n",
        "                    reg_a.u32x2.x = x_line;\n",
        "                    reg_a.u32x2.y = __shfl_sync(FULL_MASK, x_line, (laneId & 3) | 4);\n",
        "                }\n",
        "#else\n",
        "                if (laneId < 4) {\n",
        "#ifdef LOAD_X_BUFFERED\n",
        "                    reg_a.u32x2 = x_buf[0][warpId][ki % 2 * 2 + subki][laneId].u32x2;\n",
        "#endif\n",
        "                }\n",
        "#endif\n",
        "#endif\n",
        "\n",
        "#pragma unroll 2\n",
        "                for (uint32_t submi = 0; submi < 2; submi++) {\n",
        "                    uint32_t reg_c, reg_c2;\n",
        "                    if (submi == 0 && subki == 0) reg_c = reg_cs.x;\n",
        "                    else if (submi == 1 && subki == 0) reg_c = reg_cs.y;\n",
        "                    else if (submi == 0 && subki == 1) reg_c = reg_cs.z;\n",
        "                    else if (submi == 1 && subki == 1) reg_c = reg_cs.w;\n",
        "                    if (submi == 0 && subki == 0) reg_c2 = reg_cs2.x;\n",
        "                    else if (submi == 1 && subki == 0) reg_c2 = reg_cs2.y;\n",
        "                    else if (submi == 0 && subki == 1) reg_c2 = reg_cs2.z;\n",
        "                    else if (submi == 1 && subki == 1) reg_c2 = reg_cs2.w;\n",
        "\n",
        "                    // ** decode weights **\n",
        "\n",
        "#define DO_MMA\n",
        "#ifdef DO_MMA\n",
        "                    // at R = 2, 16 bit -> 8 weights -> 4 half2\n",
        "                    ditto4 reg_w;\n",
        "                    #pragma unroll\n",
        "                    for (uint32_t j = 0; j < 4; j += 1) {\n",
        "#define DO_LOOKUP\n",
        "#ifndef DO_LOOKUP\n",
        "                        reg_w.u32[0] = reg_c;\n",
        "                        reg_w.u32[1] = reg_c;\n",
        "                        reg_w.u32[2] = reg_c;\n",
        "                        reg_w.u32[3] = reg_c;\n",
        "#else\n",
        "                        uint32_t idx;\n",
        "                        if constexpr(R == 2) {\n",
        "                            idx = reg_c >> (4 * (4-j));\n",
        "                        } else if constexpr(R == 3) {\n",
        "                            idx = (j < 3) ? (reg_c >> (6 * (2-j) + 4)) : reg_c2;\n",
        "                        } else if constexpr(R == 4) {\n",
        "                            idx = (j < 3) ? (reg_c >> (8 * (2-j))) : reg_c2;\n",
        "                        }\n",
        "\n",
        "                        static_assert(L==16);\n",
        "                        idx = idx * (idx+1);\n",
        "                        uint32_t masked_idx = ((idx & 0b0111111111000000) | (laneId << 1)); // this /2 will not be elided automatically\n",
        "                        __builtin_assume(masked_idx % 2 == 0);\n",
        "#define DO_LUT\n",
        "#ifdef DO_LUT\n",
        "                        reg_w.f16x2[j] = smem_codebook[masked_idx/2];\n",
        "                        //asm(\"ld.shared.u32 %0, [%1];\" : \"=r\"(reg_w.u32[j]) : \"r\"((masked_idx * 2 + (uint16_t) smem_codebook)));\n",
        "#endif\n",
        "                        // sign flip\n",
        "                        uint32_t selector = 0b00000000'00000000'10000000'00000000;\n",
        "                        reg_w.u32[j] = reg_w.u32[j] ^ (selector & idx);\n",
        "#endif\n",
        "                    }\n",
        "\n",
        "                    //printf(\"%u: %f %f %f %f\\n\", tileIdK, __half2float(reg_w.f16x2[0].x),__half2float(reg_w.f16x2[0].y), __half2float(reg_w.f16x2[1].x),__half2float(reg_w.f16x2[1].y));\n",
        "                    asm volatile (\n",
        "                            \"mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\"\n",
        "                            \" {%0, %1, %2, %3},\"\n",
        "                            \" {%4, %5, %6, %7},\"\n",
        "                            \" {%8, %9},\"\n",
        "                            \" {%0, %1, %2, %3};\"\n",
        "                            : \"+f\"(reg_p[submi].x), \"+f\"(reg_p[submi].y), \"+f\"(reg_p[submi].z), \"+f\"(reg_p[submi].w)\n",
        "                            :  \"r\"(reg_w.u32[0]), \"r\"(reg_w.u32[1]), \"r\"(reg_w.u32[2]), \"r\"(reg_w.u32[3]),\n",
        "                            \"r\"(reg_a.u32[0]), \"r\"(reg_a.u32[1])\n",
        "                    );\n",
        "                    //printf(\"%u %u %u: %f %f %f %f\\n\", tileIdM, warpId, laneId, reg_p.x, reg_p.y, reg_p.z, reg_p.w);\n",
        "#else\n",
        "#ifdef LOAD_X\n",
        "                    reg_p.x += reg_c * reg_a.u32[0];\n",
        "                    reg_p.y += reg_c * reg_a.u32[1];\n",
        "                    reg_p.z += reg_c * reg_a.u32[0];\n",
        "                    reg_p.w += reg_c * reg_a.u32[1];\n",
        "#else\n",
        "                    reg_p.x += reg_c;\n",
        "                    reg_p.y += reg_c;\n",
        "                    reg_p.z += reg_c;\n",
        "                    reg_p.w += reg_c;\n",
        "#endif\n",
        "#endif\n",
        "                }\n",
        "\n",
        "            }\n",
        "            //if constexpr(enable_kim4_sync) {if (ki % 4 == 0) __syncthreads();} // slower with 7b even with this if constexpr thing fsr\n",
        "#define PREFETCH_X\n",
        "#ifdef LOAD_X\n",
        "#ifdef PREFETCH_X\n",
        "            if (ki % 2 == 0) {\n",
        "                prefetch((uint32_t *) (x + x_idx + x_idx_step*4));\n",
        "            }\n",
        "#endif\n",
        "#endif\n",
        "        }\n",
        "\n",
        "        __shared__ __align__(16 * 8*32) float reduce_gather[BLOCK_SIZE / WARP_SIZE][2][16];\n",
        "        if (laneId % 4 == 0) {\n",
        "            for (int pi = 0; pi < 2; pi++) {\n",
        "                reduce_gather[warpId][pi][laneId / 4] = reg_p[pi].x;\n",
        "                reduce_gather[warpId][pi][laneId / 4 + 8] = reg_p[pi].z;\n",
        "            }\n",
        "        }\n",
        "        __syncthreads();\n",
        "        float reduced = 0.0;\n",
        "        if (warpId < 1) {\n",
        "            int pi = laneId / 16;\n",
        "            for (int warpi = 0; warpi < BLOCK_SIZE / WARP_SIZE; warpi++) {\n",
        "                reduced += reduce_gather[warpi][pi][laneId % 16];\n",
        "            }\n",
        "\n",
        "            // TODO: https://forums.developer.nvidia.com/t/can-float4-be-used-for-atomicadd-efficiently/215692\n",
        "            // two rows at a time\n",
        "            float *out_tile = out + (tileIdM * 2) * f32_per_out_tile;\n",
        "            out_tile[laneId] = reduced;\n",
        "        }\n",
        "        if constexpr(m_per_block > 1) __syncthreads();\n",
        "        tileIdM += 1;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "// L: shift register bit-width\n",
        "// S: codebook index bit-width\n",
        "// R: bits per weight\n",
        "// V: log2(VQ dimension)\n",
        "template <uint32_t L, uint32_t S, uint32_t R, uint32_t V, uint32_t M, uint32_t N, uint32_t K>\n",
        "__host__ static void decompress_matvec_ptr(\n",
        "    float *__restrict__ out,                    // m-by-n\n",
        "    const uint32_t *__restrict__ compressed,    // m-by-k\n",
        "    const half2 * __restrict__ x,               // k-by-n\n",
        "    const half2 * __restrict__ codebook,\n",
        "    CUstream_st *stream\n",
        ") {\n",
        "    static_assert(L <= 16, \"Shift register should fit in uint16_t\");\n",
        "    static_assert(L >= S, \"Shift register state space must not be smaller than codebook size\");\n",
        "    static_assert(S + V >= 3, \"Codebook must have at least eight float16 elements as smem copy operates on uint4\");\n",
        "    static_assert(S + 5 + V + 1 <= 16, \"We can only use 64 KiB shared memory\"); // warpSize is 1<<5, sizeof(half) is 1<<1\n",
        "    static_assert(R == 2 || R == 3 || R == 4, \"Quantization rate = 2 or 3 or 4 for now\");\n",
        "    static_assert(V == 1, \"Always quantize two weights at a time\");\n",
        "\n",
        "    static_assert(M % MMA_M == 0);\n",
        "    static_assert(N == 1);\n",
        "    static_assert(K % MMA_K == 0);\n",
        "\n",
        "    cudaDeviceProp deviceProp;\n",
        "    cudaGetDeviceProperties(&deviceProp, 0);\n",
        "    //assert(deviceProp.multiProcessorCount == SM_COUNT);\n",
        "    //assert(deviceProp.maxThreadsPerMultiProcessor == MAX_THREADS_PER_SM);\n",
        "    assert(deviceProp.warpSize == WARP_SIZE);\n",
        "\n",
        "    //static_assert(MAX_THREADS_PER_SM % BLOCK_SIZE == 0);\n",
        "    static_assert(BLOCK_SIZE % WARP_SIZE == 0);\n",
        "\n",
        "    constexpr uint32_t gridSize = BLOCK_COUNT;\n",
        "    constexpr uint32_t blockSize = BLOCK_SIZE;\n",
        "    constexpr uint32_t smemCodebookSize = 1<<(S+5+V+1);\n",
        "    constexpr uint32_t smemReduceGatherSize = 2 * BLOCK_SIZE * sizeof(float4);\n",
        "    cudaFuncSetAttribute(kernel_decompress_matvec<L, S, R, V, M, N, K>,\n",
        "            cudaFuncAttributeMaxDynamicSharedMemorySize,\n",
        "            smemCodebookSize);\n",
        "\n",
        "    kernel_decompress_matvec<L, S, R, V, M, N, K><<<gridSize, blockSize, smemCodebookSize, stream>>>(out, compressed, x, codebook);\n",
        "\n",
        "    gpuErrchk(cudaPeekAtLastError());\n",
        "}\n"
      ],
      "metadata": {
        "id": "_D42o2qho0jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -o inference /content/inference.cu\n",
        "!./inference\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2vGtEshqKX7",
        "outputId": "efdeb39f-5734-4702-a231-cf92c979969a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[K/content/inference.cu:11:10:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kc10/cuda/CUDAStream.h: No such file or directory\n",
            "   11 | #include \u001b[01;31m\u001b[K<c10/cuda/CUDAStream.h>\u001b[m\u001b[K\n",
            "      |          \u001b[01;31m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "compilation terminated.\n",
            "/bin/bash: line 1: ./inference: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Cornell-RelaxML/qtip.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpYWTqfZqXAK",
        "outputId": "9e4e42bc-23ca-4120-9851-f5dbea3c575c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'qtip'...\n",
            "remote: Enumerating objects: 612, done.\u001b[K\n",
            "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 612 (delta 29), reused 27 (delta 25), pack-reused 554 (from 1)\u001b[K\n",
            "Receiving objects: 100% (612/612), 884.50 KiB | 13.01 MiB/s, done.\n",
            "Resolving deltas: 100% (363/363), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/qtip/qtip-kernels/src\n",
        "!nvcc -arch=sm_75 -o inference inference.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhOnpTLkqZUr",
        "outputId": "2efde43d-29cd-47e2-efa7-33102568905b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qtip/qtip-kernels/src\n",
            "\u001b[01m\u001b[Kinference.cu:11:10:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kc10/cuda/CUDAStream.h: No such file or directory\n",
            "   11 | #include \u001b[01;31m\u001b[K<c10/cuda/CUDAStream.h>\u001b[m\u001b[K\n",
            "      |          \u001b[01;31m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "compilation terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/qtip/qtip-kernels/src\n",
        "!nvcc -o inference inference.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeKvBRYzrIkL",
        "outputId": "02b3f70b-5e86-4240-ade8-6524f02a492d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qtip/qtip-kernels/src\n",
            "\u001b[01m\u001b[Kinference.cu:11:10:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kc10/cuda/CUDAStream.h: No such file or directory\n",
            "   11 | #include \u001b[01;31m\u001b[K<c10/cuda/CUDAStream.h>\u001b[m\u001b[K\n",
            "      |          \u001b[01;31m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "compilation terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # يجب أن يكون True\n",
        "print(torch.__version__)  # تحقق من الإصدار\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G14R0q7PrLB0",
        "outputId": "3d199ecd-aecf-4ece-be28-eed4b1900284"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "2.5.1+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/qtip/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5O3Rk7JSrVab",
        "outputId": "9c41be8b-e83d-4f89-db01-b3bd7ddd859c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate==0.34.2 (from -r /content/qtip/requirements.txt (line 1))\n",
            "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting cuda_python==12.6.0 (from -r /content/qtip/requirements.txt (line 2))\n",
            "  Downloading cuda_python-12.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting datasets==2.20.0 (from -r /content/qtip/requirements.txt (line 3))\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting fast_hadamard_transform==1.0.4.post1 (from -r /content/qtip/requirements.txt (line 4))\n",
            "  Downloading fast_hadamard_transform-1.0.4.post1.tar.gz (6.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting glog==0.3.1 (from -r /content/qtip/requirements.txt (line 5))\n",
            "  Downloading glog-0.3.1-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting huggingface_hub==0.24.0 (from -r /content/qtip/requirements.txt (line 6))\n",
            "  Downloading huggingface_hub-0.24.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting lm_eval==0.4.4 (from -r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading lm_eval-0.4.4-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==2.1.2 (from -r /content/qtip/requirements.txt (line 8))\n",
            "  Downloading numpy-2.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy==1.14.1 in /usr/local/lib/python3.11/dist-packages (from -r /content/qtip/requirements.txt (line 9)) (1.14.1)\n",
            "Collecting setuptools==69.5.1 (from -r /content/qtip/requirements.txt (line 10))\n",
            "  Downloading setuptools-69.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting torch==2.4.0 (from -r /content/qtip/requirements.txt (line 11))\n",
            "  Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting tqdm==4.66.4 (from -r /content/qtip/requirements.txt (line 12))\n",
            "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.45.2 (from -r /content/qtip/requirements.txt (line 13))\n",
            "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.34.2->-r /content/qtip/requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.34.2->-r /content/qtip/requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate==0.34.2->-r /content/qtip/requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.34.2->-r /content/qtip/requirements.txt (line 1)) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (18.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets==2.20.0->-r /content/qtip/requirements.txt (line 3))\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets==2.20.0->-r /content/qtip/requirements.txt (line 3))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (2.32.3)\n",
            "Collecting xxhash (from datasets==2.20.0->-r /content/qtip/requirements.txt (line 3))\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets==2.20.0->-r /content/qtip/requirements.txt (line 3))\n",
            "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3))\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (3.11.13)\n",
            "Collecting ninja (from fast_hadamard_transform==1.0.4.post1->-r /content/qtip/requirements.txt (line 4))\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting python-gflags>=3.1 (from glog==0.3.1->-r /content/qtip/requirements.txt (line 5))\n",
            "  Downloading python-gflags-3.1.2.tar.gz (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from glog==0.3.1->-r /content/qtip/requirements.txt (line 5)) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.24.0->-r /content/qtip/requirements.txt (line 6)) (4.12.2)\n",
            "Collecting evaluate (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting jsonlines (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (2.10.2)\n",
            "Requirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (0.14.0)\n",
            "Collecting pybind11>=2.6.2 (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting pytablewriter (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting rouge-score>=0.0.4 (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacrebleu>=1.5.0 (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (1.6.1)\n",
            "Collecting sqlitedict (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tqdm-multiprocess (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (0.23.0)\n",
            "Collecting word2number (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (10.6.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11)) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11)) (9.1.0.70)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11))\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11))\n",
            "  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.2->-r /content/qtip/requirements.txt (line 13)) (2024.11.6)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.2->-r /content/qtip/requirements.txt (line 13))\n",
            "  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->-r /content/qtip/requirements.txt (line 11)) (12.4.127)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (1.18.3)\n",
            "INFO: pip is looking at multiple versions of peft to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting peft>=0.2.0 (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (2025.1.31)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (3.9.1)\n",
            "Collecting portalocker (from sacrebleu>=1.5.0->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.5.0->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (5.3.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.4.0->-r /content/qtip/requirements.txt (line 11)) (3.0.2)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets==2.20.0->-r /content/qtip/requirements.txt (line 3))\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (2025.1)\n",
            "Collecting DataProperty<2,>=1.1.0 (from pytablewriter->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading pathvalidate-3.2.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.4.0->-r /content/qtip/requirements.txt (line 11)) (1.3.0)\n",
            "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.11/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (5.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score>=0.0.4->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (8.1.8)\n",
            "Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cuda_python-12.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.0/25.0 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glog-0.3.1-py2.py3-none-any.whl (7.8 kB)\n",
            "Downloading huggingface_hub-0.24.0-py3-none-any.whl (419 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.0/419.0 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_eval-0.4.4-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-69.5.1-py3-none-any.whl (894 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m894.6/894.6 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl (797.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.3/797.3 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Downloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\n",
            "Downloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\n",
            "Downloading pathvalidate-3.2.3-py3-none-any.whl (24 kB)\n",
            "Downloading tabledata-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\n",
            "Downloading typepy-1.3.4-py3-none-any.whl (31 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: fast_hadamard_transform, python-gflags, rouge-score, sqlitedict, word2number\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for fast_hadamard_transform (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for fast_hadamard_transform\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for fast_hadamard_transform\n",
            "  Building wheel for python-gflags (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-gflags: filename=python_gflags-3.1.2-py3-none-any.whl size=57370 sha256=e7485b5e7c71f0554578c105959f10cc6244b93ddb668902fc15091f8b756b3c\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/32/87/be6254803d81af495c135b8b662d4a076e7a91dc7766f05a25\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=206969a5c4255fbb078ba918218f8c5d611435ef151501c725f5973c4a0d782f\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=960a5bc3b50f7bca7d9a249b7f67d22103230847f2d58116feb449072b95c3b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/63/89/7210274f9b7fb033b8f22671f64c0e0b55083d30c3c046a3ff\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=3fc6cdcd4846d0c7d7b616139c1fd0c2180fc0ed8fe167d81e516ff22bcf28bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/ef/ae/073b491b14d25e2efafcffca9e16b2ee6d114ec5c643ba4f06\n",
            "Successfully built python-gflags rouge-score sqlitedict word2number\n",
            "Failed to build fast_hadamard_transform\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (fast_hadamard_transform)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import torch; print(torch.__path__)\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk7fGoiNreZ7",
        "outputId": "8faa08c5-9a30-4e31-f106-2e35c8431bf5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/usr/local/lib/python3.11/dist-packages/torch']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -I/usr/local/lib/python3.11/dist-packages/torch/include -o inference inference.cu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVMYhZv3r2zL",
        "outputId": "93623191-a0c7-4132-9c0b-5c079d88c018"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01minference.cu(225)\u001b[0m: \u001b[01;35mwarning\u001b[0m #191-D: type qualifier is meaningless on cast type\n",
            "          load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx, laneId, reg_cs_next, reg_cs2_next);\n",
            "                          ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01minference.cu(266)\u001b[0m: \u001b[01;35mwarning\u001b[0m #191-D: type qualifier is meaningless on cast type\n",
            "              load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx + (1 - ki % 2) * u16_per_tile_block, laneId, reg_cs_next, reg_cs2_next);\n",
            "                              ^\n",
            "\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/barrier:24\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/pipeline:66\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinference.cu:8\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/std/barrier:15:4:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K#error \"CUDA synchronization primitives are only supported for sm_70 and up.\"\n",
            "   15 | #  \u001b[01;31m\u001b[Kerror\u001b[m\u001b[K \"CUDA synchronization primitives are only supported for sm_70 and up.\"\n",
            "      |    \u001b[01;31m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda-12.5/targets/x86_64-linux/include/cuda/std/detail/libcxx/include/atomic:734\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda-12.5/targets/x86_64-linux/include/cuda/std/detail/libcxx/include/barrier:71\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/std/barrier:22\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/barrier:24\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/pipeline:66\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinference.cu:8\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda-12.5/targets/x86_64-linux/include/cuda/std/detail/libcxx/include/support/atomic/atomic_cuda.h:12:4:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K#error \"CUDA atomics are only supported for sm_60 and up on *nix and sm_70 and up on Windows.\"\n",
            "   12 | #  \u001b[01;31m\u001b[Kerror\u001b[m\u001b[K \"CUDA atomics are only supported for sm_60 and up on *nix and sm_70 and up on Windows.\"\n",
            "      |    \u001b[01;31m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda-12.5/targets/x86_64-linux/include/cuda/std/detail/libcxx/include/barrier:487\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/std/barrier:22\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/barrier:24\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/pipeline:66\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinference.cu:8\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda-12.5/targets/x86_64-linux/include/cuda/std/detail/libcxx/include/__cuda/barrier.h:19:4:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K#error \"CUDA synchronization primitives are only supported for sm_70 and up.\"\n",
            "   19 | #  \u001b[01;31m\u001b[Kerror\u001b[m\u001b[K \"CUDA synchronization primitives are only supported for sm_70 and up.\"\n",
            "      |    \u001b[01;31m\u001b[K^~~~~\u001b[m\u001b[K\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -I/usr/local/lib/python3.11/dist-packages/torch/include \\\n",
        "      -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include \\\n",
        "      -o inference inference.cu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpmjU5SdsQIU",
        "outputId": "71c80258-ffaf-485a-e3b6-cc7e7df80f50"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01minference.cu(225)\u001b[0m: \u001b[01;35mwarning\u001b[0m #191-D: type qualifier is meaningless on cast type\n",
            "          load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx, laneId, reg_cs_next, reg_cs2_next);\n",
            "                          ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01minference.cu(266)\u001b[0m: \u001b[01;35mwarning\u001b[0m #191-D: type qualifier is meaningless on cast type\n",
            "              load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx + (1 - ki % 2) * u16_per_tile_block, laneId, reg_cs_next, reg_cs2_next);\n",
            "                              ^\n",
            "\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/barrier:24\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/pipeline:66\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinference.cu:8\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/std/barrier:15:4:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K#error \"CUDA synchronization primitives are only supported for sm_70 and up.\"\n",
            "   15 | #  \u001b[01;31m\u001b[Kerror\u001b[m\u001b[K \"CUDA synchronization primitives are only supported for sm_70 and up.\"\n",
            "      |    \u001b[01;31m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda-12.5/targets/x86_64-linux/include/cuda/std/detail/libcxx/include/atomic:734\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda-12.5/targets/x86_64-linux/include/cuda/std/detail/libcxx/include/barrier:71\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/std/barrier:22\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/barrier:24\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/pipeline:66\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinference.cu:8\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda-12.5/targets/x86_64-linux/include/cuda/std/detail/libcxx/include/support/atomic/atomic_cuda.h:12:4:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K#error \"CUDA atomics are only supported for sm_60 and up on *nix and sm_70 and up on Windows.\"\n",
            "   12 | #  \u001b[01;31m\u001b[Kerror\u001b[m\u001b[K \"CUDA atomics are only supported for sm_60 and up on *nix and sm_70 and up on Windows.\"\n",
            "      |    \u001b[01;31m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda-12.5/targets/x86_64-linux/include/cuda/std/detail/libcxx/include/barrier:487\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/std/barrier:22\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/barrier:24\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/pipeline:66\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinference.cu:8\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda-12.5/targets/x86_64-linux/include/cuda/std/detail/libcxx/include/__cuda/barrier.h:19:4:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K#error \"CUDA synchronization primitives are only supported for sm_70 and up.\"\n",
            "   19 | #  \u001b[01;31m\u001b[Kerror\u001b[m\u001b[K \"CUDA synchronization primitives are only supported for sm_70 and up.\"\n",
            "      |    \u001b[01;31m\u001b[K^~~~~\u001b[m\u001b[K\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_70 -I/usr/local/lib/python3.11/dist-packages/torch/include -o inference inference.cu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vzwi9XQksQij",
        "outputId": "46149064-0191-44be-a1ed-4cfc63f473b0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01minference.cu(225)\u001b[0m: \u001b[01;35mwarning\u001b[0m #191-D: type qualifier is meaningless on cast type\n",
            "          load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx, laneId, reg_cs_next, reg_cs2_next);\n",
            "                          ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01minference.cu(266)\u001b[0m: \u001b[01;35mwarning\u001b[0m #191-D: type qualifier is meaningless on cast type\n",
            "              load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx + (1 - ki % 2) * u16_per_tile_block, laneId, reg_cs_next, reg_cs2_next);\n",
            "                              ^\n",
            "\n",
            "/usr/bin/ld: /usr/lib/gcc/x86_64-linux-gnu/11/../../../x86_64-linux-gnu/Scrt1.o: in function `_start':\n",
            "(.text+0x1b): undefined reference to `main'\n",
            "collect2: error: ld returned 1 exit status\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -I/usr/local/lib/python3.11/dist-packages/torch/include -o inference inference.cu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRD6Iw13sZQT",
        "outputId": "e8670cf8-0bd1-4a1c-e9f3-d72ff34baa64"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01minference.cu(225)\u001b[0m: \u001b[01;35mwarning\u001b[0m #191-D: type qualifier is meaningless on cast type\n",
            "          load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx, laneId, reg_cs_next, reg_cs2_next);\n",
            "                          ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01minference.cu(266)\u001b[0m: \u001b[01;35mwarning\u001b[0m #191-D: type qualifier is meaningless on cast type\n",
            "              load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx + (1 - ki % 2) * u16_per_tile_block, laneId, reg_cs_next, reg_cs2_next);\n",
            "                              ^\n",
            "\n",
            "/usr/bin/ld: /usr/lib/gcc/x86_64-linux-gnu/11/../../../x86_64-linux-gnu/Scrt1.o: in function `_start':\n",
            "(.text+0x1b): undefined reference to `main'\n",
            "collect2: error: ld returned 1 exit status\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hh-uBA8Hsh8j",
        "outputId": "12a338be-6266-4acc-911a-ff95cbb4ef62"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Cornell-RelaxML/qtip.git\n",
        "!cd qtip/qtip-kernels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mN39qgp9tgzU",
        "outputId": "34cccb0a-fb97-49ab-ce67-4e26043bf903"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'qtip'...\n",
            "remote: Enumerating objects: 612, done.\u001b[K\n",
            "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 612 (delta 29), reused 27 (delta 25), pack-reused 554 (from 1)\u001b[K\n",
            "Receiving objects: 100% (612/612), 884.50 KiB | 11.79 MiB/s, done.\n",
            "Resolving deltas: 100% (363/363), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/qtip/qtip-kernels/src"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5bpht1qt7O1",
        "outputId": "9f7e2a50-905c-4b2b-e329-f192d83150a4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qtip/qtip-kernels/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o inference.out src/inference.cu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXDWHDKFtq9L",
        "outputId": "45580602-92ee-43c1-c0d2-a3fd833e69b3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Ksrc/inference.cu: No such file or directory\n",
            "compilation terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o inference.out src/inference.cu -lcudart -rdc=true\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFqn1rlPtuxz",
        "outputId": "fab9f7c4-6e3c-4563-f06a-0a45f391198e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Ksrc/inference.cu: No such file or directory\n",
            "compilation terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o inference.out src/inference.cu -lcudart -rdc=true\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_W3Krkgt4G7",
        "outputId": "dcecdd8c-bba8-4d86-b55e-31e7282bd68a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Ksrc/inference.cu: No such file or directory\n",
            "compilation terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -c src/inference.cu -o inference.o\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT6jCCqiuCFT",
        "outputId": "c7158afe-0bbf-4f47-82f1-0fb01fc97b2e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Ksrc/inference.cu: No such file or directory\n",
            "compilation terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./inference.out\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE13RyKpuGtL",
        "outputId": "a01aa1b4-b666-4cae-f6e8-8ce28c005441"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: ./inference.out: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd qtip/qtip-kernels && ls src/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOgPJ0DeuI_L",
        "outputId": "bab0048b-c6b0-47b0-fcc2-58e63b655038"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "baseline.py  inference.cu  inference.h\tMakefile  qtip_torch.cu  test.cu  wrapper.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o inference.out src/inference.cu -lcudart -rdc=true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dEwpt--uQNj",
        "outputId": "1ffbb4aa-162d-4ca4-f00a-ec9bc6dddec2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Ksrc/inference.cu: No such file or directory\n",
            "compilation terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o inference.out /content/qtip/qtip-kernels/src/inference.cu -lcudart -rdc=true\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlBbah_huWWM",
        "outputId": "d366602d-d89c-495d-c98d-0ac705e20a41"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[K/content/qtip/qtip-kernels/src/inference.cu:11:10:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kc10/cuda/CUDAStream.h: No such file or directory\n",
            "   11 | #include \u001b[01;31m\u001b[K<c10/cuda/CUDAStream.h>\u001b[m\u001b[K\n",
            "      |          \u001b[01;31m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "compilation terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls -l\n",
        "!nvcc --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUAfwHlCubRz",
        "outputId": "682b640e-3980-4a92-8979-1f3ca66fe13f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qtip/qtip-kernels/src\n",
            "total 84\n",
            "-rw-r--r--  1 root root   497 Mar 14 03:19 baseline.py\n",
            "-rw-r--r--  1 root root 19479 Mar 14 03:29 inference.cu\n",
            "-rw-r--r--  1 root root  1269 Mar 14 03:32 inference.h\n",
            "-rw-r--r--  1 root root   432 Mar 14 03:19 Makefile\n",
            "drwxr-xr-x 10 root root  4096 Mar 14 03:33 qtip\n",
            "-rw-r--r--  1 root root 19596 Mar 14 03:19 qtip_torch.cu\n",
            "-rw-r--r--  1 root root  1955 Mar 14 03:19 test.cu\n",
            "-rw-r--r--  1 root root 20645 Mar 14 03:19 wrapper.cpp\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /usr -name \"CUDAStream.h\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQWAk3-AukZr",
        "outputId": "62227dc6-a94a-4c26-e32a-f91f4c8fae38"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/include/c10/cuda/CUDAStream.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import torch; print(torch.__path__)\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OL-53cryut5j",
        "outputId": "f1299d71-12bd-4b5f-f9a0-b5de1e8e1613"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/usr/local/lib/python3.11/dist-packages/torch']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o inference.out inference.cu -I/usr/local/lib/python3.10/dist-packages/torch/include -lcudart -rdc=true\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB2EBPl3u4wL",
        "outputId": "a887d642-258a-4610-fbf0-9a9bdc2aa6f1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kinference.cu:11:10:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kc10/cuda/CUDAStream.h: No such file or directory\n",
            "   11 | #include \u001b[01;31m\u001b[K<c10/cuda/CUDAStream.h>\u001b[m\u001b[K\n",
            "      |          \u001b[01;31m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "compilation terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env CKPT=/content/llama_quantized\n",
        "%env HF=/content/llama_hf\n",
        "%env LOG=/content/llama_logs\n",
        "%env HESS=/content/hessian_data\n",
        "%env CKPT=/content/llama_quantized\n",
        "%env HF=/content/llama_hf\n",
        "%env LOG=/content/llama_logs\n",
        "%env HESS=/content/hessian_data\n",
        "\n",
        "!mkdir -p $CKPT $HF $LOG\n",
        "!export HESS=/content/hessian_data\n",
        "\n",
        "!mkdir -p $CKPT $HF $LOG\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YA3Xrqa3vJy8",
        "outputId": "8a556248-70d1-4c50-dbd5-333f2c324d02"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CKPT=/content/llama_quantized\n",
            "env: HF=/content/llama_hf\n",
            "env: LOG=/content/llama_logs\n",
            "env: HESS=/content/hessian_data\n",
            "env: CKPT=/content/llama_quantized\n",
            "env: HF=/content/llama_hf\n",
            "env: LOG=/content/llama_logs\n",
            "env: HESS=/content/hessian_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m quantize_llama.quantize_finetune_llama \\\n",
        "       --save_path $CKPT/2_7b_2bit \\\n",
        "       --codebook bitshift \\\n",
        "       --base_model meta-llama/Llama-2-7b-hf \\\n",
        "       --in_hess_path $HESS \\\n",
        "       --scale_override 0.9 \\\n",
        "       --ft_epochs 5 \\\n",
        "       --td_x 16 \\\n",
        "       --td_y 16 \\\n",
        "       --L 16 \\\n",
        "       --K 2 \\\n",
        "       --V 2 \\\n",
        "       --decode_mode quantlut_sym \\\n",
        "       --tlut_bits 9 \\\n",
        "       >> $LOG/2_7b_2bit 2>&1\n"
      ],
      "metadata": {
        "id": "BCBgTKfPvfH0"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m quantize_llama.hfize_llama \\\n",
        "       --quantized_path $CKPT/2_7b_2bit \\\n",
        "       --hf_output_path $HF/2_7b_2bit \\\n",
        "       >> $LOG/2_7b_2bit 2>&1\n"
      ],
      "metadata": {
        "id": "yP9QO26Lvlc0"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m quantize_llama.finetune_e2e_llama \\\n",
        "       --base_model meta-llama/Llama-2-7b-hf \\\n",
        "       --hf_path $HF/2_7b_2bit \\\n",
        "       --devset_size 640 \\\n",
        "       --ft_valid_size 128 \\\n",
        "       --ft_epochs 4 \\\n",
        "       --ft_update_freq 4 \\\n",
        "       --ft_bs 2 \\\n",
        "       --ctx_size 4096 \\\n",
        "       --ft_train_lut \\\n",
        "       --hf_output_path $HF/2_7b_2bit \\\n",
        "       >> $LOG/2_7b_2bit 2>&1\n"
      ],
      "metadata": {
        "id": "2U4ZemjZvp3s"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m eval.eval_ppl  --hf_path $HF/2_7b_2bit >> $LOG/2_7b_2bit 2>&1\n",
        "\n"
      ],
      "metadata": {
        "id": "1s_Fv4oIvtvE"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m eval.eval_zeroshot \\\n",
        "       --tasks arc_challenge,arc_easy,boolq,piqa,winogrande \\\n",
        "       --batch_size 16 \\\n",
        "       --hf_path $HF/2_7b_2bit \\\n",
        "       >> $LOG/2_7b_2bit 2>&1\n"
      ],
      "metadata": {
        "id": "QnbWmkg9vv8E"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!export PYTHONPATH=$PYTHONPATH:/content/qtip\n"
      ],
      "metadata": {
        "id": "XYkab2wYvz7U"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# **حدد المسار إلى النموذج المضغوط**\n",
        "model_path = \"relaxml/Llama-2-7b-chat-QTIP-2Bit\"  # استبدلها بمسارك الفعلي\n",
        "\n",
        "# **تحميل التوكنيزر والنموذج**\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "# **اختبار النموذج**\n",
        "input_text = \"مرحبا! كيف يمكنني مساعدتك اليوم؟\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "# **توليد النص**\n",
        "output = model.generate(input_ids, max_length=100)\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"📝 النص المولّد:\", generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669,
          "referenced_widgets": [
            "a64eab1f5d844025829fd73d3b4ef713",
            "237775f85157454d8a67e1d8b52debad",
            "4bc2e34107854bf7b810a97c284605a9",
            "4aa01775d4054302968daad5c2b1ee6a",
            "e6ea9f05581f47779c0d45a2fe50a63e",
            "aad9af6d1c8d41719d6a6897156cd384",
            "3ae35104cef64397bb799ccd96865ecd",
            "839b20dddac249e8a6adfaee40ed0bc0",
            "6e050ef8144e4921921a8c36f5e83bf7",
            "7d9f01cff54b493abdeebf5ff429e624",
            "60f7e36bd24f4bddb7ed95590db1a6d5"
          ]
        },
        "id": "VKlel70zv26E",
        "outputId": "94ae8e4e-447f-4f2e-df28-fecb66c77e87"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/912 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a64eab1f5d844025829fd73d3b4ef713"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Can't load tokenizer for 'relaxml/Llama-2-7b-chat-QTIP-2Bit'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'relaxml/Llama-2-7b-chat-QTIP-2Bit' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-df3dbdded049>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# **تحميل التوكنيزر والنموذج**\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    954\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2018\u001b[0m         \u001b[0;31m# loaded directly from the GGUF file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2019\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_file_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull_file_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgguf_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2020\u001b[0;31m             raise EnvironmentError(\n\u001b[0m\u001b[1;32m   2021\u001b[0m                 \u001b[0;34mf\"Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m                 \u001b[0;34m\"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'relaxml/Llama-2-7b-chat-QTIP-2Bit'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'relaxml/Llama-2-7b-chat-QTIP-2Bit' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fk_8yCJdw7Sk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4a67260538ed478d889b55c935606e14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8cef9c21e9ae4c7b8ca9d922b08584af",
              "IPY_MODEL_d128928506b74d2b801c832cba858a78",
              "IPY_MODEL_3a41c60d226e4a0eab69e8d17c85da2e"
            ],
            "layout": "IPY_MODEL_8a44155379d94bb7b9e9973dc5c4ca65"
          }
        },
        "8cef9c21e9ae4c7b8ca9d922b08584af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c609ede361e455087abe4ac9dd920bf",
            "placeholder": "​",
            "style": "IPY_MODEL_1d14c591ff7e4a77b41b63e38fa11d56",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "d128928506b74d2b801c832cba858a78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8bbe38b75874091864fbab9a08b4050",
            "max": 50500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da4a0388294e4c70a25819e6302f9cc9",
            "value": 50500
          }
        },
        "3a41c60d226e4a0eab69e8d17c85da2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8410b90e380e49bf96b0acb053a94076",
            "placeholder": "​",
            "style": "IPY_MODEL_db988e5066c9427c93049476bf6f9296",
            "value": " 50.5k/50.5k [00:00&lt;00:00, 4.68MB/s]"
          }
        },
        "8a44155379d94bb7b9e9973dc5c4ca65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c609ede361e455087abe4ac9dd920bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d14c591ff7e4a77b41b63e38fa11d56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8bbe38b75874091864fbab9a08b4050": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da4a0388294e4c70a25819e6302f9cc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8410b90e380e49bf96b0acb053a94076": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db988e5066c9427c93049476bf6f9296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f6356e237f14f72abeaf44b14221bad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b6e2e4de1b934d64adc3aad666640b01",
              "IPY_MODEL_efa244a934cd4c28a82eebc186503bf7",
              "IPY_MODEL_f1902974934244c59f1c6c6d6fd2a9d7"
            ],
            "layout": "IPY_MODEL_a9e2cc5a6d6a4973b57b18e10614fc9c"
          }
        },
        "b6e2e4de1b934d64adc3aad666640b01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57a211df26474a29b0406c5b2aec8b77",
            "placeholder": "​",
            "style": "IPY_MODEL_0f7e089da39944ebabbc6bb999e89d1d",
            "value": "tokenizer.json: 100%"
          }
        },
        "efa244a934cd4c28a82eebc186503bf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0950cb7877744cb81326c73c2225f2e",
            "max": 9085657,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_907e8de6739c411c800dd4d277b66237",
            "value": 9085657
          }
        },
        "f1902974934244c59f1c6c6d6fd2a9d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1c8f1e74f274c1697ac67b4a43486ce",
            "placeholder": "​",
            "style": "IPY_MODEL_b68308d14bdf491d8044a27ffbfce322",
            "value": " 9.09M/9.09M [00:00&lt;00:00, 18.0MB/s]"
          }
        },
        "a9e2cc5a6d6a4973b57b18e10614fc9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57a211df26474a29b0406c5b2aec8b77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f7e089da39944ebabbc6bb999e89d1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0950cb7877744cb81326c73c2225f2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "907e8de6739c411c800dd4d277b66237": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1c8f1e74f274c1697ac67b4a43486ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b68308d14bdf491d8044a27ffbfce322": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e97f5314a8fb4fc5b158ff5f55b35e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90fb1aa45ddb41658d37453082b93bb4",
              "IPY_MODEL_15a0033542c847ba9f427d206fbfe457",
              "IPY_MODEL_98f4ed70ed9d414b89e6bac6a80bfc6a"
            ],
            "layout": "IPY_MODEL_9f20ffd82c934c44a73d3fe1d73a7d21"
          }
        },
        "90fb1aa45ddb41658d37453082b93bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_115528dec6a146fab11dae9f975f6299",
            "placeholder": "​",
            "style": "IPY_MODEL_cbbd31fa2b544015a54d16f9b4619773",
            "value": "config.json: 100%"
          }
        },
        "15a0033542c847ba9f427d206fbfe457": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43aea904d5ea430288f01d1dda42e93d",
            "max": 1117,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36855321a79c439aa950f7c35d19ae8e",
            "value": 1117
          }
        },
        "98f4ed70ed9d414b89e6bac6a80bfc6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71a29153ebff4775afa5966959f3d673",
            "placeholder": "​",
            "style": "IPY_MODEL_a0ea013b9fd54853ad76c6b0144449b0",
            "value": " 1.12k/1.12k [00:00&lt;00:00, 68.0kB/s]"
          }
        },
        "9f20ffd82c934c44a73d3fe1d73a7d21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "115528dec6a146fab11dae9f975f6299": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbbd31fa2b544015a54d16f9b4619773": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43aea904d5ea430288f01d1dda42e93d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36855321a79c439aa950f7c35d19ae8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71a29153ebff4775afa5966959f3d673": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0ea013b9fd54853ad76c6b0144449b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bdf0c80a75cf4753a678780935f087e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_524a8931b4074ed196687117f4dc5cbe",
              "IPY_MODEL_0d2ed0b0c9f0483dad36a867c2f15396",
              "IPY_MODEL_4d0d65f0535c4c1881a63040f2530e64"
            ],
            "layout": "IPY_MODEL_bcc79f8c6ce049419d62038d6e96a1e7"
          }
        },
        "524a8931b4074ed196687117f4dc5cbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb1e48a38393422f8b15504937dc70d7",
            "placeholder": "​",
            "style": "IPY_MODEL_53131e9d2a7e4526984bdfd27da09cfc",
            "value": "model.safetensors: 100%"
          }
        },
        "0d2ed0b0c9f0483dad36a867c2f15396": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8af7b822097c4d7191927344f3b890b8",
            "max": 3852517272,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c64e639577cc4a7fb9def4026900d1a1",
            "value": 3852517272
          }
        },
        "4d0d65f0535c4c1881a63040f2530e64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bda83ee17df4a9eb67c0d5249951b0a",
            "placeholder": "​",
            "style": "IPY_MODEL_9fdd6074307b4eafac304cbf6cb4d446",
            "value": " 3.85G/3.85G [00:47&lt;00:00, 81.2MB/s]"
          }
        },
        "bcc79f8c6ce049419d62038d6e96a1e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb1e48a38393422f8b15504937dc70d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53131e9d2a7e4526984bdfd27da09cfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8af7b822097c4d7191927344f3b890b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c64e639577cc4a7fb9def4026900d1a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5bda83ee17df4a9eb67c0d5249951b0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fdd6074307b4eafac304cbf6cb4d446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a64eab1f5d844025829fd73d3b4ef713": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_237775f85157454d8a67e1d8b52debad",
              "IPY_MODEL_4bc2e34107854bf7b810a97c284605a9",
              "IPY_MODEL_4aa01775d4054302968daad5c2b1ee6a"
            ],
            "layout": "IPY_MODEL_e6ea9f05581f47779c0d45a2fe50a63e"
          }
        },
        "237775f85157454d8a67e1d8b52debad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aad9af6d1c8d41719d6a6897156cd384",
            "placeholder": "​",
            "style": "IPY_MODEL_3ae35104cef64397bb799ccd96865ecd",
            "value": "config.json: 100%"
          }
        },
        "4bc2e34107854bf7b810a97c284605a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_839b20dddac249e8a6adfaee40ed0bc0",
            "max": 912,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e050ef8144e4921921a8c36f5e83bf7",
            "value": 912
          }
        },
        "4aa01775d4054302968daad5c2b1ee6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d9f01cff54b493abdeebf5ff429e624",
            "placeholder": "​",
            "style": "IPY_MODEL_60f7e36bd24f4bddb7ed95590db1a6d5",
            "value": " 912/912 [00:00&lt;00:00, 20.7kB/s]"
          }
        },
        "e6ea9f05581f47779c0d45a2fe50a63e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aad9af6d1c8d41719d6a6897156cd384": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ae35104cef64397bb799ccd96865ecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "839b20dddac249e8a6adfaee40ed0bc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e050ef8144e4921921a8c36f5e83bf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d9f01cff54b493abdeebf5ff429e624": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60f7e36bd24f4bddb7ed95590db1a6d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtOtNaN4mqRV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UgfGVK5imuY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2r92xzqRmubx",
        "outputId": "aa400718-c146-432a-b51c-7cde02ec8092"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.45.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import os\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# Check if CUDA is available\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "\n",
        "# Load the tokenizer\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "print(\"Tokenizer loaded successfully!\")\n",
        "\n",
        "# Load the model without additional quantization since it's already quantized to 2-bit\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Function to generate responses\n",
        "def generate_response(prompt, max_new_tokens=512):\n",
        "    # Format prompt for Llama 3.1\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    # Format input for model\n",
        "    input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate response\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "\n",
        "    # Decode response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract assistant's response (remove the prompt)\n",
        "    assistant_response = response[len(input_text):]\n",
        "\n",
        "    return assistant_response\n",
        "\n",
        "# Example usage\n",
        "prompt = \"Explain quantum computing in simple terms.\"\n",
        "print(\"\\nGenerating response to:\", prompt)\n",
        "response = generate_response(prompt)\n",
        "print(\"\\nModel response:\", response)\n",
        "\n",
        "# Interactive mode\n",
        "def interactive_chat():\n",
        "    print(\"\\n=== Interactive Chat Mode ===\")\n",
        "    print(\"Type 'exit' to end the conversation.\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Ending chat session.\")\n",
        "            break\n",
        "\n",
        "        print(\"\\nGenerating response...\")\n",
        "        response = generate_response(user_input)\n",
        "        print(f\"\\nAssistant: {response}\")\n",
        "\n",
        "# Start interactive chat\n",
        "if __name__ == \"__main__\":\n",
        "    interactive_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356,
          "referenced_widgets": [
            "4a67260538ed478d889b55c935606e14",
            "8cef9c21e9ae4c7b8ca9d922b08584af",
            "d128928506b74d2b801c832cba858a78",
            "3a41c60d226e4a0eab69e8d17c85da2e",
            "8a44155379d94bb7b9e9973dc5c4ca65",
            "0c609ede361e455087abe4ac9dd920bf",
            "1d14c591ff7e4a77b41b63e38fa11d56",
            "d8bbe38b75874091864fbab9a08b4050",
            "da4a0388294e4c70a25819e6302f9cc9",
            "8410b90e380e49bf96b0acb053a94076",
            "db988e5066c9427c93049476bf6f9296",
            "8f6356e237f14f72abeaf44b14221bad",
            "b6e2e4de1b934d64adc3aad666640b01",
            "efa244a934cd4c28a82eebc186503bf7",
            "f1902974934244c59f1c6c6d6fd2a9d7",
            "a9e2cc5a6d6a4973b57b18e10614fc9c",
            "57a211df26474a29b0406c5b2aec8b77",
            "0f7e089da39944ebabbc6bb999e89d1d",
            "a0950cb7877744cb81326c73c2225f2e",
            "907e8de6739c411c800dd4d277b66237",
            "c1c8f1e74f274c1697ac67b4a43486ce",
            "b68308d14bdf491d8044a27ffbfce322",
            "e97f5314a8fb4fc5b158ff5f55b35e44",
            "90fb1aa45ddb41658d37453082b93bb4",
            "15a0033542c847ba9f427d206fbfe457",
            "98f4ed70ed9d414b89e6bac6a80bfc6a",
            "9f20ffd82c934c44a73d3fe1d73a7d21",
            "115528dec6a146fab11dae9f975f6299",
            "cbbd31fa2b544015a54d16f9b4619773",
            "43aea904d5ea430288f01d1dda42e93d",
            "36855321a79c439aa950f7c35d19ae8e",
            "71a29153ebff4775afa5966959f3d673",
            "a0ea013b9fd54853ad76c6b0144449b0",
            "bdf0c80a75cf4753a678780935f087e2",
            "524a8931b4074ed196687117f4dc5cbe",
            "0d2ed0b0c9f0483dad36a867c2f15396",
            "4d0d65f0535c4c1881a63040f2530e64",
            "bcc79f8c6ce049419d62038d6e96a1e7",
            "cb1e48a38393422f8b15504937dc70d7",
            "53131e9d2a7e4526984bdfd27da09cfc",
            "8af7b822097c4d7191927344f3b890b8",
            "c64e639577cc4a7fb9def4026900d1a1",
            "5bda83ee17df4a9eb67c0d5249951b0a",
            "9fdd6074307b4eafac304cbf6cb4d446"
          ]
        },
        "id": "fSxwm-_bmueZ",
        "outputId": "da7bb004-85f5-4983-fbf3-7a36d2441169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n",
            "Loading tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a67260538ed478d889b55c935606e14"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f6356e237f14f72abeaf44b14221bad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer loaded successfully!\n",
            "Loading model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e97f5314a8fb4fc5b158ff5f55b35e44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.85G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bdf0c80a75cf4753a678780935f087e2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#include <cstdio>\n",
        "#include <cassert>\n",
        "#include <climits>\n",
        "\n",
        "#include <cstdlib>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cuda/pipeline>\n",
        "#include <cuda_fp16.h>\n",
        "#include <mma.h>\n",
        "#include <c10/cuda/CUDAStream.h>\n",
        "\n",
        "#include \"inference.h\"\n",
        "\n",
        "using namespace nvcuda;\n",
        "\n",
        "\n",
        "#define CHECK_CUDA(x)           TORCH_CHECK(x.is_cuda(), #x \" must be a CUDA tensor\")\n",
        "#define CHECK_CONTIGUOUS(x)     TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_INPUT(x)          do { CHECK_CUDA(x); CHECK_CONTIGUOUS(x); } while (false)\n",
        "\n",
        "#define BLOCKS_PER_SM 1\n",
        "#define MMA_M                   16\n",
        "#define MMA_N                   8\n",
        "#define MMA_K                   16\n",
        "\n",
        "#define BLOCK_COUNT             128\n",
        "//#define MAX_THREADS_PER_SM      2048\n",
        "#define WARP_SIZE               32\n",
        "#define BLOCK_SIZE              1024\n",
        "#define WARPS_PER_BLOCK         (BLOCK_SIZE/WARP_SIZE)\n",
        "\n",
        "#define PREFETCHW               4\n",
        "#define PREFETCHX               4\n",
        "#define BLOCKS_PER_SM           1\n",
        "\n",
        "#define FULL_MASK               0xFFFFFFFFU\n",
        "\n",
        "__inline__ __device__ uint32_t ld_cs(const uint32_t* p)\n",
        "{\n",
        "    uint32_t out;\n",
        "    asm(\"ld.global.cs.u32 %0, [%1];\" : \"=r\"(out) : \"l\"(p));\n",
        "    return out;\n",
        "}\n",
        "\n",
        "__inline__ __device__ uint2 ld_cs(const uint2* p)\n",
        "{\n",
        "    uint2 out;\n",
        "    asm(\"ld.global.cs.v2.u32 {%0, %1}, [%2];\" : \"=r\"(out.x), \"=r\"(out.y) : \"l\"(p));\n",
        "    //asm(\"ld.weak.global.cs.L2::256B.v2.u32 {%0, %1}, [%2];\" : \"=r\"(out.x), \"=r\"(out.y) : \"l\"(p));\n",
        "    // the compiler doesn't know how to infer load(p) and load(p+4096) from loop unrolling with this :(\n",
        "    return out;\n",
        "}\n",
        "__inline__ __device__ uint3 ld_cs(const uint3* p)\n",
        "{\n",
        "    uint3 out;\n",
        "    asm(\"ld.global.cs.u32 %0, [%1];\" : \"=r\"(out.x) : \"l\"(p));\n",
        "    asm(\"ld.global.cs.u32 %0, [%1+4];\" : \"=r\"(out.y) : \"l\"(p));\n",
        "    asm(\"ld.global.cs.u32 %0, [%1+8];\" : \"=r\"(out.z) : \"l\"(p));\n",
        "    return out;\n",
        "}\n",
        "__inline__ __device__ uint4 ld_cs(const uint4* p)\n",
        "{\n",
        "    uint4 out;\n",
        "    asm(\"ld.global.cs.v4.u32 {%0, %1, %2, %3}, [%4];\" : \"=r\"(out.x), \"=r\"(out.y), \"=r\"(out.z), \"=r\"(out.w) : \"l\"(p));\n",
        "    return out;\n",
        "}\n",
        "__inline__ __device__ uint2 ld_x(const uint32_t* p, uint32_t x_idx, int subki)\n",
        "{\n",
        "    uint2 out;\n",
        "    // the indexing is written as int32 math instead of lsu constant offset because\n",
        "    // apparently using lsu offset adds lots of MIO pressure!\n",
        "    if (subki == 0) {\n",
        "        asm(\"ld.global.L1::evict_last.u32 %0, [%1];\" : \"=r\"(out.x) : \"l\"(p+x_idx));\n",
        "        asm(\"ld.global.L1::evict_last.u32 %0, [%1];\" : \"=r\"(out.y) : \"l\"(p+(x_idx+4)));\n",
        "    } else {\n",
        "        asm(\"ld.global.L1::evict_last.u32 %0, [%1];\" : \"=r\"(out.x) : \"l\"(p+(x_idx+8)));\n",
        "        asm(\"ld.global.L1::evict_last.u32 %0, [%1];\" : \"=r\"(out.y) : \"l\"(p+(x_idx+12)));\n",
        "    }\n",
        "    return out;\n",
        "}\n",
        "__inline__ __device__ uint32_t ld_x(const uint32_t* p)\n",
        "{\n",
        "    uint32_t out;\n",
        "    asm(\"ld.global.L1::evict_last.u32 %0, [%1];\" : \"=r\"(out) : \"l\"(p));\n",
        "    return out;\n",
        "}\n",
        "\n",
        "__inline__ __device__ void prefetch(uint32_t *a){\n",
        "    asm(\"prefetch.global.L1 [%0];\"::\"l\"(a));\n",
        "}\n",
        "\n",
        "#define LD_CS\n",
        "template <uint32_t R>\n",
        "__device__ inline void load_reg_cs(const uint16_t *__restrict__ compressed, int weight_idx, uint32_t laneId, uint4 &reg_cs_next, uint4 &reg_cs2_next) {\n",
        "    if constexpr(R == 2) {\n",
        "#ifdef LD_CS\n",
        "        ditto2 reg_load = {.u32x2 = ld_cs((uint2 *) &compressed[weight_idx])};\n",
        "#else\n",
        "        ditto2 reg_load = {.u16x4 = *((ushort4 * )(compressed + weight_idx))};\n",
        "#endif\n",
        "        uint32_t next1 = __shfl_sync(FULL_MASK, reg_load.u32x2.x, laneId + 1);\n",
        "        uint32_t next2 = __shfl_sync(FULL_MASK, reg_load.u32x2.y, laneId + 1);\n",
        "        reg_cs_next.x = __byte_perm(next1, reg_load.u32x2.x, 0x5410);\n",
        "        reg_cs_next.y = __byte_perm(next1, reg_load.u32x2.x, 0x7632);\n",
        "        reg_cs_next.z = __byte_perm(next2, reg_load.u32x2.y, 0x5410);\n",
        "        reg_cs_next.w = __byte_perm(next2, reg_load.u32x2.y, 0x7632);\n",
        "    } else if constexpr(R == 3) {\n",
        "#ifdef LD_CS\n",
        "        uint3 reg_load = ld_cs((uint3 *) &compressed[weight_idx]);\n",
        "        uint32_t reg_load1 = reg_load.x, reg_load2 = reg_load.y, reg_load3 = reg_load.z;\n",
        "#else\n",
        "        uint32_t reg_load1 = *((uint32_t *) &compressed[weight_idx]);\n",
        "        uint32_t reg_load2 = *((uint32_t *) &compressed[weight_idx + 2]);\n",
        "        uint32_t reg_load3 = *((uint32_t *) &compressed[weight_idx + 4]);\n",
        "#endif\n",
        "\n",
        "        uint32_t reg_24_1 = reg_load1 & 0xffffff;\n",
        "        uint32_t reg_24_2 = ((reg_load1 >> 24) | (reg_load2 << 8)) & 0xffffff;\n",
        "        uint32_t reg_24_3 = ((reg_load2 >> 16) | (reg_load3 << 16)) & 0xffffff;\n",
        "        uint32_t reg_24_4 = (reg_load3 >> 8) & 0xffffff;\n",
        "\n",
        "        // send high 16 bits to prev thread\n",
        "        uint32_t pack1 = (reg_24_1 >> 8) | ((reg_24_2 << 8) & 0xffff0000);\n",
        "        uint32_t pack3 = (reg_24_3 >> 8) | ((reg_24_4 << 8) & 0xffff0000);\n",
        "\n",
        "        // receive high 16 bits from next thread\n",
        "        uint32_t next1 = __shfl_sync(FULL_MASK, pack1, laneId + 1);\n",
        "        uint32_t next3 = __shfl_sync(FULL_MASK, pack3, laneId + 1);\n",
        "\n",
        "        reg_cs_next.x = __byte_perm(next1, reg_24_1, 0x6541);\n",
        "        reg_cs_next.y = __byte_perm(next1, reg_24_2, 0x6543);\n",
        "        reg_cs_next.z = __byte_perm(next3, reg_24_3, 0x6541);\n",
        "        reg_cs_next.w = __byte_perm(next3, reg_24_4, 0x6543);\n",
        "\n",
        "        reg_cs2_next.x = ((next1 >> 6) & 0b11'1111'1111) | (reg_24_1 << 10);\n",
        "        reg_cs2_next.y = ((next1 >> (6 + 16) & 0b11'1111'1111)) | (reg_24_2 << 10);\n",
        "        reg_cs2_next.z = ((next3 >> 6) & 0b11'1111'1111) | (reg_24_3 << 10);\n",
        "        reg_cs2_next.w = ((next3 >> (6 + 16) & 0b11'1111'1111)) | (reg_24_4 << 10);\n",
        "    } else if constexpr(R == 4) {\n",
        "#ifdef LD_CS\n",
        "        uint4 reg_load = ld_cs((uint4 *) &compressed[weight_idx]);\n",
        "#else\n",
        "        uint4 reg_load = *((uint4 *) &compressed[weight_idx]);\n",
        "#endif\n",
        "        uint32_t reg_load1 = reg_load.x, reg_load2 = reg_load.y, reg_load3 = reg_load.z, reg_load4 = reg_load.w;\n",
        "\n",
        "        // send high 16 bits to prev thread\n",
        "        uint32_t pack1 = (reg_load1 >> 16) | (reg_load2 & 0xffff0000);\n",
        "        uint32_t pack3 = (reg_load3 >> 16) | (reg_load4 & 0xffff0000);\n",
        "\n",
        "        uint32_t next1 = __shfl_sync(FULL_MASK, pack1, laneId + 1);\n",
        "        uint32_t next3 = __shfl_sync(FULL_MASK, pack3, laneId + 1);\n",
        "\n",
        "        reg_cs_next.x = reg_load1;\n",
        "        reg_cs_next.y = reg_load2;\n",
        "        reg_cs_next.z = reg_load3;\n",
        "        reg_cs_next.w = reg_load4;\n",
        "\n",
        "        reg_cs2_next.x = __byte_perm(next1, reg_load1, 0x0041);\n",
        "        reg_cs2_next.y = __byte_perm(next1, reg_load2, 0x0043);\n",
        "        reg_cs2_next.z = __byte_perm(next3, reg_load3, 0x0041);\n",
        "        reg_cs2_next.w = __byte_perm(next3, reg_load4, 0x0043);\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "template <uint32_t L, uint32_t S, uint32_t R, uint32_t V, uint32_t M, uint32_t N, uint32_t K>\n",
        "__global__ static void\n",
        "__launch_bounds__(BLOCK_SIZE, 1)\n",
        "kernel_decompress_matvec(\n",
        "    float *__restrict__ out,\n",
        "    const uint32_t *__restrict__ compressed,\n",
        "    const half2 *__restrict__ x,\n",
        "    const half2 *__restrict__ codebook\n",
        ") {\n",
        "        // ** load codebook **\n",
        "    extern __shared__ __align__(1<<(5+V+1)) half2 smem_codebook[];\n",
        "\n",
        "    // ** cursed indexing math **\n",
        "\n",
        "    uint32_t threadId = threadIdx.x;\n",
        "    uint32_t laneId = threadIdx.x % WARP_SIZE;\n",
        "    uint32_t warpId = threadId / WARP_SIZE;\n",
        "    uint32_t blockId = blockIdx.x;\n",
        "\n",
        "    constexpr uint32_t tileCountM = M / MMA_M;\n",
        "    constexpr uint32_t tileCountK = K / MMA_K;\n",
        "\n",
        "    constexpr uint32_t warps_per_block = BLOCK_SIZE / WARP_SIZE;\n",
        "\n",
        "#define ROUND_UP(a, b) ((a + b - 1) / b)\n",
        "\n",
        "    static_assert (tileCountM % 2 == 0);\n",
        "    constexpr uint32_t m_per_block = ROUND_UP(tileCountM, (2 * BLOCK_COUNT));\n",
        "    // tiles are iterated along k in groups of 2\n",
        "    //static_assert (tileCountK >= warps_per_block * 2);\n",
        "    constexpr uint32_t k_per_block = tileCountK / (warps_per_block * 4) * 2;\n",
        "    // we sync at ki%4==0, make sure this is safe\n",
        "    //constexpr bool enable_kim4_sync = !(M == 4096 && K==4096) && (tileCountK % (warps_per_block * 2) == 0 || k_per_block % 4 != 0);\n",
        "    // some warps have more k tiles\n",
        "    static_assert((tileCountK % (warps_per_block * 4)) % 4 == 0);\n",
        "    uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
        "\n",
        "    constexpr uint32_t u16_per_compressed_tile = MMA_M * MMA_K * R / 16;\n",
        "    static_assert((MMA_M * MMA_K * R) % 16 == 0);\n",
        "    constexpr uint32_t f16x2_per_x_tile = MMA_K / 2;\n",
        "    constexpr uint32_t f32_per_out_tile = MMA_M;\n",
        "\n",
        "    uint32_t tileIdM = m_per_block * blockId;\n",
        "\n",
        "    constexpr uint32_t weight_block = 4;\n",
        "    constexpr uint32_t u16_per_tile_block = u16_per_compressed_tile * weight_block; // one tile block per warp at a time\n",
        "    constexpr uint32_t weight_step = warps_per_block * u16_per_tile_block;\n",
        "    constexpr uint32_t weight_row_step = tileCountK * u16_per_compressed_tile * 2;  // 2 rows of tiles\n",
        "\n",
        "\n",
        "\n",
        "    for (uint32_t mi = 0; mi < m_per_block; mi+=1) {\n",
        "        if (tileIdM * 2 >= tileCountM) return;\n",
        "        // ** load weight, start loop **\n",
        "        int weight_idx = tileIdM * weight_row_step + warpId * u16_per_tile_block * 2 + laneId * (u16_per_tile_block / WARP_SIZE);\n",
        "        uint4 reg_cs_next = {};\n",
        "        uint4 reg_cs2_next = {};\n",
        "        load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx, laneId, reg_cs_next, reg_cs2_next);\n",
        "        uint4 reg_cs;\n",
        "        uint4 reg_cs2;\n",
        "\n",
        "        // define acc\n",
        "        float4 reg_p[2] = {};\n",
        "\n",
        "#define LOAD_X_BUFFERED\n",
        "#ifdef PERMUTE_K\n",
        "        uint32_t x_idx = warpId * f16x4_per_x_tile*2 + laneId;\n",
        "        uint32_t x_idx_step = warps_per_block * f16x4_per_x_tile * 2;\n",
        "#else\n",
        "#if !defined(LOAD_X_SHUFFLE) && !defined(LOAD_X_BUFFERED)\n",
        "        uint32_t x_idx = warpId * f16x2_per_x_tile * 2 + laneId;  // every warp does 2 k tiles per iteration\n",
        "        uint32_t x_idx_step = warps_per_block * f16x2_per_x_tile * 2;\n",
        "#else\n",
        "        uint32_t x_idx = warpId * f16x2_per_x_tile * 4 + laneId;  // every warp does 4 k tiles per iteration\n",
        "        uint32_t x_idx_step = warps_per_block * f16x2_per_x_tile * 4;\n",
        "#endif\n",
        "#endif\n",
        "        if (mi == 0) {\n",
        "#define DO_LOAD_CODEBOOK\n",
        "#ifdef DO_LOAD_CODEBOOK\n",
        "            uint32_t my_cb_idx = threadIdx.x & 0x1ff;\n",
        "            half2 my_codebook_element = codebook[my_cb_idx];\n",
        "            for (uint32_t i = 0; i < 32; i+= 2) {\n",
        "                smem_codebook[(my_cb_idx << 5)|(i ^ (threadIdx.x & 0x1f) ^ (threadIdx.x >> 9))] = my_codebook_element;\n",
        "            }\n",
        "            // for (uint32_t i = 0; i < 32; i+= 1) { assert(smem_codebook[(my_cb_idx << 5) + i] == my_codebook_element); }\n",
        "            __syncthreads();\n",
        "#endif\n",
        "        }\n",
        "\n",
        "        __shared__ ditto2 x_buf[2][BLOCK_SIZE / WARP_SIZE][4][4];\n",
        "        uint32_t x_line;\n",
        "#pragma unroll 4\n",
        "        for (uint32_t ki = 0; ki < this_warp_k; ki += 1) {\n",
        "            // load this 2x2 block of weight tiles\n",
        "            if (ki + 1 != this_warp_k && ki % 2 == 1) weight_idx += weight_step * 2; // fixme: this costs 10GB/s\n",
        "            reg_cs = reg_cs_next;\n",
        "            reg_cs2 = reg_cs2_next;\n",
        "            load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx + (1 - ki % 2) * u16_per_tile_block, laneId, reg_cs_next, reg_cs2_next);\n",
        "\n",
        "#define LOAD_X\n",
        "#ifdef LOAD_X\n",
        "#ifdef LOAD_X_BUFFERED\n",
        "            if (ki % 2 == 0) {\n",
        "                __syncwarp();\n",
        "                x_buf[0][warpId][laneId / 8][laneId % 4].u32[(laneId % 8) / 4] = ld_x(reinterpret_cast<const uint32_t *>(x) + x_idx);\n",
        "                __syncwarp();\n",
        "                x_idx += x_idx_step;\n",
        "            }\n",
        "#else\n",
        "#ifdef LOAD_X_SHUFFLE\n",
        "            if (ki % 2 == 0) {\n",
        "                x_line = ld_x(((uint32_t *) x) + x_idx);\n",
        "                x_idx += x_idx_step;\n",
        "            }\n",
        "#endif\n",
        "#endif\n",
        "#endif\n",
        "\n",
        "#pragma unroll 2\n",
        "            for (uint32_t subki = 0; subki < 2; subki += 1) {\n",
        "                // load activations\n",
        "                // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#matrix-fragments-for-mma-m16n8k16-with-floating-point-type\n",
        "                ditto2 reg_a;\n",
        "#define LD_X\n",
        "#ifdef LOAD_X\n",
        "#ifdef LOAD_X_SHUFFLE\n",
        "                uint32_t x_subki = (ki % 2 * 2 + subki);\n",
        "                if (x_subki != 0) {\n",
        "                    reg_a.u32x2.x = __shfl_sync(FULL_MASK, x_line, (laneId & 3) | (8 * x_subki));\n",
        "                    reg_a.u32x2.y = __shfl_sync(FULL_MASK, x_line, (laneId & 3) | (4 | (8 * x_subki)));\n",
        "                } else {\n",
        "                    reg_a.u32x2.x = x_line;\n",
        "                    reg_a.u32x2.y = __shfl_sync(FULL_MASK, x_line, (laneId & 3) | 4);\n",
        "                }\n",
        "#else\n",
        "                if (laneId < 4) {\n",
        "#ifdef LOAD_X_BUFFERED\n",
        "                    reg_a.u32x2 = x_buf[0][warpId][ki % 2 * 2 + subki][laneId].u32x2;\n",
        "#endif\n",
        "                }\n",
        "#endif\n",
        "#endif\n",
        "\n",
        "#pragma unroll 2\n",
        "                for (uint32_t submi = 0; submi < 2; submi++) {\n",
        "                    uint32_t reg_c, reg_c2;\n",
        "                    if (submi == 0 && subki == 0) reg_c = reg_cs.x;\n",
        "                    else if (submi == 1 && subki == 0) reg_c = reg_cs.y;\n",
        "                    else if (submi == 0 && subki == 1) reg_c = reg_cs.z;\n",
        "                    else if (submi == 1 && subki == 1) reg_c = reg_cs.w;\n",
        "                    if (submi == 0 && subki == 0) reg_c2 = reg_cs2.x;\n",
        "                    else if (submi == 1 && subki == 0) reg_c2 = reg_cs2.y;\n",
        "                    else if (submi == 0 && subki == 1) reg_c2 = reg_cs2.z;\n",
        "                    else if (submi == 1 && subki == 1) reg_c2 = reg_cs2.w;\n",
        "\n",
        "                    // ** decode weights **\n",
        "\n",
        "#define DO_MMA\n",
        "#ifdef DO_MMA\n",
        "                    // at R = 2, 16 bit -> 8 weights -> 4 half2\n",
        "                    ditto4 reg_w;\n",
        "                    #pragma unroll\n",
        "                    for (uint32_t j = 0; j < 4; j += 1) {\n",
        "#define DO_LOOKUP\n",
        "#ifndef DO_LOOKUP\n",
        "                        reg_w.u32[0] = reg_c;\n",
        "                        reg_w.u32[1] = reg_c;\n",
        "                        reg_w.u32[2] = reg_c;\n",
        "                        reg_w.u32[3] = reg_c;\n",
        "#else\n",
        "                        uint32_t idx;\n",
        "                        if constexpr(R == 2) {\n",
        "                            idx = reg_c >> (4 * (4-j));\n",
        "                        } else if constexpr(R == 3) {\n",
        "                            idx = (j < 3) ? (reg_c >> (6 * (2-j) + 4)) : reg_c2;\n",
        "                        } else if constexpr(R == 4) {\n",
        "                            idx = (j < 3) ? (reg_c >> (8 * (2-j))) : reg_c2;\n",
        "                        }\n",
        "\n",
        "                        static_assert(L==16);\n",
        "                        idx = idx * (idx+1);\n",
        "                        uint32_t masked_idx = ((idx & 0b0111111111000000) | (laneId << 1)); // this /2 will not be elided automatically\n",
        "                        __builtin_assume(masked_idx % 2 == 0);\n",
        "#define DO_LUT\n",
        "#ifdef DO_LUT\n",
        "                        reg_w.f16x2[j] = smem_codebook[masked_idx/2];\n",
        "                        //asm(\"ld.shared.u32 %0, [%1];\" : \"=r\"(reg_w.u32[j]) : \"r\"((masked_idx * 2 + (uint16_t) smem_codebook)));\n",
        "#endif\n",
        "                        // sign flip\n",
        "                        uint32_t selector = 0b00000000'00000000'10000000'00000000;\n",
        "                        reg_w.u32[j] = reg_w.u32[j] ^ (selector & idx);\n",
        "#endif\n",
        "                    }\n",
        "\n",
        "                    //printf(\"%u: %f %f %f %f\\n\", tileIdK, __half2float(reg_w.f16x2[0].x),__half2float(reg_w.f16x2[0].y), __half2float(reg_w.f16x2[1].x),__half2float(reg_w.f16x2[1].y));\n",
        "                    asm volatile (\n",
        "                            \"mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\"\n",
        "                            \" {%0, %1, %2, %3},\"\n",
        "                            \" {%4, %5, %6, %7},\"\n",
        "                            \" {%8, %9},\"\n",
        "                            \" {%0, %1, %2, %3};\"\n",
        "                            : \"+f\"(reg_p[submi].x), \"+f\"(reg_p[submi].y), \"+f\"(reg_p[submi].z), \"+f\"(reg_p[submi].w)\n",
        "                            :  \"r\"(reg_w.u32[0]), \"r\"(reg_w.u32[1]), \"r\"(reg_w.u32[2]), \"r\"(reg_w.u32[3]),\n",
        "                            \"r\"(reg_a.u32[0]), \"r\"(reg_a.u32[1])\n",
        "                    );\n",
        "                    //printf(\"%u %u %u: %f %f %f %f\\n\", tileIdM, warpId, laneId, reg_p.x, reg_p.y, reg_p.z, reg_p.w);\n",
        "#else\n",
        "#ifdef LOAD_X\n",
        "                    reg_p.x += reg_c * reg_a.u32[0];\n",
        "                    reg_p.y += reg_c * reg_a.u32[1];\n",
        "                    reg_p.z += reg_c * reg_a.u32[0];\n",
        "                    reg_p.w += reg_c * reg_a.u32[1];\n",
        "#else\n",
        "                    reg_p.x += reg_c;\n",
        "                    reg_p.y += reg_c;\n",
        "                    reg_p.z += reg_c;\n",
        "                    reg_p.w += reg_c;\n",
        "#endif\n",
        "#endif\n",
        "                }\n",
        "\n",
        "            }\n",
        "            //if constexpr(enable_kim4_sync) {if (ki % 4 == 0) __syncthreads();} // slower with 7b even with this if constexpr thing fsr\n",
        "#define PREFETCH_X\n",
        "#ifdef LOAD_X\n",
        "#ifdef PREFETCH_X\n",
        "            if (ki % 2 == 0) {\n",
        "                prefetch((uint32_t *) (x + x_idx + x_idx_step*4));\n",
        "            }\n",
        "#endif\n",
        "#endif\n",
        "        }\n",
        "\n",
        "        __shared__ __align__(16 * 8*32) float reduce_gather[BLOCK_SIZE / WARP_SIZE][2][16];\n",
        "        if (laneId % 4 == 0) {\n",
        "            for (int pi = 0; pi < 2; pi++) {\n",
        "                reduce_gather[warpId][pi][laneId / 4] = reg_p[pi].x;\n",
        "                reduce_gather[warpId][pi][laneId / 4 + 8] = reg_p[pi].z;\n",
        "            }\n",
        "        }\n",
        "        __syncthreads();\n",
        "        float reduced = 0.0;\n",
        "        if (warpId < 1) {\n",
        "            int pi = laneId / 16;\n",
        "            for (int warpi = 0; warpi < BLOCK_SIZE / WARP_SIZE; warpi++) {\n",
        "                reduced += reduce_gather[warpi][pi][laneId % 16];\n",
        "            }\n",
        "\n",
        "            // TODO: https://forums.developer.nvidia.com/t/can-float4-be-used-for-atomicadd-efficiently/215692\n",
        "            // two rows at a time\n",
        "            float *out_tile = out + (tileIdM * 2) * f32_per_out_tile;\n",
        "            out_tile[laneId] = reduced;\n",
        "        }\n",
        "        if constexpr(m_per_block > 1) __syncthreads();\n",
        "        tileIdM += 1;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "// L: shift register bit-width\n",
        "// S: codebook index bit-width\n",
        "// R: bits per weight\n",
        "// V: log2(VQ dimension)\n",
        "template <uint32_t L, uint32_t S, uint32_t R, uint32_t V, uint32_t M, uint32_t N, uint32_t K>\n",
        "__host__ static void decompress_matvec_ptr(\n",
        "    float *__restrict__ out,                    // m-by-n\n",
        "    const uint32_t *__restrict__ compressed,    // m-by-k\n",
        "    const half2 * __restrict__ x,               // k-by-n\n",
        "    const half2 * __restrict__ codebook,\n",
        "    CUstream_st *stream\n",
        ") {\n",
        "    static_assert(L <= 16, \"Shift register should fit in uint16_t\");\n",
        "    static_assert(L >= S, \"Shift register state space must not be smaller than codebook size\");\n",
        "    static_assert(S + V >= 3, \"Codebook must have at least eight float16 elements as smem copy operates on uint4\");\n",
        "    static_assert(S + 5 + V + 1 <= 16, \"We can only use 64 KiB shared memory\"); // warpSize is 1<<5, sizeof(half) is 1<<1\n",
        "    static_assert(R == 2 || R == 3 || R == 4, \"Quantization rate = 2 or 3 or 4 for now\");\n",
        "    static_assert(V == 1, \"Always quantize two weights at a time\");\n",
        "\n",
        "    static_assert(M % MMA_M == 0);\n",
        "    static_assert(N == 1);\n",
        "    static_assert(K % MMA_K == 0);\n",
        "\n",
        "    cudaDeviceProp deviceProp;\n",
        "    cudaGetDeviceProperties(&deviceProp, 0);\n",
        "    //assert(deviceProp.multiProcessorCount == SM_COUNT);\n",
        "    //assert(deviceProp.maxThreadsPerMultiProcessor == MAX_THREADS_PER_SM);\n",
        "    assert(deviceProp.warpSize == WARP_SIZE);\n",
        "\n",
        "    //static_assert(MAX_THREADS_PER_SM % BLOCK_SIZE == 0);\n",
        "    static_assert(BLOCK_SIZE % WARP_SIZE == 0);\n",
        "\n",
        "    constexpr uint32_t gridSize = BLOCK_COUNT;\n",
        "    constexpr uint32_t blockSize = BLOCK_SIZE;\n",
        "    constexpr uint32_t smemCodebookSize = 1<<(S+5+V+1);\n",
        "    constexpr uint32_t smemReduceGatherSize = 2 * BLOCK_SIZE * sizeof(float4);\n",
        "    cudaFuncSetAttribute(kernel_decompress_matvec<L, S, R, V, M, N, K>,\n",
        "            cudaFuncAttributeMaxDynamicSharedMemorySize,\n",
        "            smemCodebookSize);\n",
        "\n",
        "    kernel_decompress_matvec<L, S, R, V, M, N, K><<<gridSize, blockSize, smemCodebookSize, stream>>>(out, compressed, x, codebook);\n",
        "\n",
        "    gpuErrchk(cudaPeekAtLastError());\n",
        "}\n"
      ],
      "metadata": {
        "id": "_D42o2qho0jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -o inference /content/inference.cu\n",
        "!./inference\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2vGtEshqKX7",
        "outputId": "efdeb39f-5734-4702-a231-cf92c979969a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[K/content/inference.cu:11:10:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kc10/cuda/CUDAStream.h: No such file or directory\n",
            "   11 | #include \u001b[01;31m\u001b[K<c10/cuda/CUDAStream.h>\u001b[m\u001b[K\n",
            "      |          \u001b[01;31m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "compilation terminated.\n",
            "/bin/bash: line 1: ./inference: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Cornell-RelaxML/qtip.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpYWTqfZqXAK",
        "outputId": "9e4e42bc-23ca-4120-9851-f5dbea3c575c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'qtip'...\n",
            "remote: Enumerating objects: 612, done.\u001b[K\n",
            "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 612 (delta 29), reused 27 (delta 25), pack-reused 554 (from 1)\u001b[K\n",
            "Receiving objects: 100% (612/612), 884.50 KiB | 13.01 MiB/s, done.\n",
            "Resolving deltas: 100% (363/363), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/qtip/qtip-kernels/src\n",
        "!nvcc -arch=sm_75 -o inference inference.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhOnpTLkqZUr",
        "outputId": "2efde43d-29cd-47e2-efa7-33102568905b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qtip/qtip-kernels/src\n",
            "\u001b[01m\u001b[Kinference.cu:11:10:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kc10/cuda/CUDAStream.h: No such file or directory\n",
            "   11 | #include \u001b[01;31m\u001b[K<c10/cuda/CUDAStream.h>\u001b[m\u001b[K\n",
            "      |          \u001b[01;31m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "compilation terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/qtip/qtip-kernels/src\n",
        "!nvcc -o inference inference.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeKvBRYzrIkL",
        "outputId": "02b3f70b-5e86-4240-ade8-6524f02a492d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qtip/qtip-kernels/src\n",
            "\u001b[01m\u001b[Kinference.cu:11:10:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kc10/cuda/CUDAStream.h: No such file or directory\n",
            "   11 | #include \u001b[01;31m\u001b[K<c10/cuda/CUDAStream.h>\u001b[m\u001b[K\n",
            "      |          \u001b[01;31m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "compilation terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # يجب أن يكون True\n",
        "print(torch.__version__)  # تحقق من الإصدار\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G14R0q7PrLB0",
        "outputId": "3d199ecd-aecf-4ece-be28-eed4b1900284"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "2.5.1+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/qtip/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5O3Rk7JSrVab",
        "outputId": "9c41be8b-e83d-4f89-db01-b3bd7ddd859c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate==0.34.2 (from -r /content/qtip/requirements.txt (line 1))\n",
            "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting cuda_python==12.6.0 (from -r /content/qtip/requirements.txt (line 2))\n",
            "  Downloading cuda_python-12.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting datasets==2.20.0 (from -r /content/qtip/requirements.txt (line 3))\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting fast_hadamard_transform==1.0.4.post1 (from -r /content/qtip/requirements.txt (line 4))\n",
            "  Downloading fast_hadamard_transform-1.0.4.post1.tar.gz (6.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting glog==0.3.1 (from -r /content/qtip/requirements.txt (line 5))\n",
            "  Downloading glog-0.3.1-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting huggingface_hub==0.24.0 (from -r /content/qtip/requirements.txt (line 6))\n",
            "  Downloading huggingface_hub-0.24.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting lm_eval==0.4.4 (from -r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading lm_eval-0.4.4-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==2.1.2 (from -r /content/qtip/requirements.txt (line 8))\n",
            "  Downloading numpy-2.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy==1.14.1 in /usr/local/lib/python3.11/dist-packages (from -r /content/qtip/requirements.txt (line 9)) (1.14.1)\n",
            "Collecting setuptools==69.5.1 (from -r /content/qtip/requirements.txt (line 10))\n",
            "  Downloading setuptools-69.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting torch==2.4.0 (from -r /content/qtip/requirements.txt (line 11))\n",
            "  Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting tqdm==4.66.4 (from -r /content/qtip/requirements.txt (line 12))\n",
            "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.45.2 (from -r /content/qtip/requirements.txt (line 13))\n",
            "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.34.2->-r /content/qtip/requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.34.2->-r /content/qtip/requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate==0.34.2->-r /content/qtip/requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.34.2->-r /content/qtip/requirements.txt (line 1)) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (18.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets==2.20.0->-r /content/qtip/requirements.txt (line 3))\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets==2.20.0->-r /content/qtip/requirements.txt (line 3))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (2.32.3)\n",
            "Collecting xxhash (from datasets==2.20.0->-r /content/qtip/requirements.txt (line 3))\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets==2.20.0->-r /content/qtip/requirements.txt (line 3))\n",
            "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3))\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (3.11.13)\n",
            "Collecting ninja (from fast_hadamard_transform==1.0.4.post1->-r /content/qtip/requirements.txt (line 4))\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting python-gflags>=3.1 (from glog==0.3.1->-r /content/qtip/requirements.txt (line 5))\n",
            "  Downloading python-gflags-3.1.2.tar.gz (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from glog==0.3.1->-r /content/qtip/requirements.txt (line 5)) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.24.0->-r /content/qtip/requirements.txt (line 6)) (4.12.2)\n",
            "Collecting evaluate (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting jsonlines (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (2.10.2)\n",
            "Requirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (0.14.0)\n",
            "Collecting pybind11>=2.6.2 (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting pytablewriter (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting rouge-score>=0.0.4 (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacrebleu>=1.5.0 (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (1.6.1)\n",
            "Collecting sqlitedict (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tqdm-multiprocess (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (0.23.0)\n",
            "Collecting word2number (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (10.6.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11)) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11)) (9.1.0.70)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11))\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch==2.4.0->-r /content/qtip/requirements.txt (line 11))\n",
            "  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.2->-r /content/qtip/requirements.txt (line 13)) (2024.11.6)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.2->-r /content/qtip/requirements.txt (line 13))\n",
            "  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->-r /content/qtip/requirements.txt (line 11)) (12.4.127)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (1.18.3)\n",
            "INFO: pip is looking at multiple versions of peft to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting peft>=0.2.0 (from lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (2025.1.31)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (3.9.1)\n",
            "Collecting portalocker (from sacrebleu>=1.5.0->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.5.0->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (5.3.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.4.0->-r /content/qtip/requirements.txt (line 11)) (3.0.2)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets==2.20.0->-r /content/qtip/requirements.txt (line 3))\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.20.0->-r /content/qtip/requirements.txt (line 3)) (2025.1)\n",
            "Collecting DataProperty<2,>=1.1.0 (from pytablewriter->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading pathvalidate-3.2.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7))\n",
            "  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.4.0->-r /content/qtip/requirements.txt (line 11)) (1.3.0)\n",
            "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.11/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (5.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score>=0.0.4->lm_eval==0.4.4->-r /content/qtip/requirements.txt (line 7)) (8.1.8)\n",
            "Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cuda_python-12.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.0/25.0 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glog-0.3.1-py2.py3-none-any.whl (7.8 kB)\n",
            "Downloading huggingface_hub-0.24.0-py3-none-any.whl (419 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.0/419.0 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_eval-0.4.4-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-69.5.1-py3-none-any.whl (894 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m894.6/894.6 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl (797.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.3/797.3 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Downloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\n",
            "Downloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\n",
            "Downloading pathvalidate-3.2.3-py3-none-any.whl (24 kB)\n",
            "Downloading tabledata-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\n",
            "Downloading typepy-1.3.4-py3-none-any.whl (31 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: fast_hadamard_transform, python-gflags, rouge-score, sqlitedict, word2number\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for fast_hadamard_transform (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for fast_hadamard_transform\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for fast_hadamard_transform\n",
            "  Building wheel for python-gflags (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-gflags: filename=python_gflags-3.1.2-py3-none-any.whl size=57370 sha256=e7485b5e7c71f0554578c105959f10cc6244b93ddb668902fc15091f8b756b3c\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/32/87/be6254803d81af495c135b8b662d4a076e7a91dc7766f05a25\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=206969a5c4255fbb078ba918218f8c5d611435ef151501c725f5973c4a0d782f\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=960a5bc3b50f7bca7d9a249b7f67d22103230847f2d58116feb449072b95c3b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/63/89/7210274f9b7fb033b8f22671f64c0e0b55083d30c3c046a3ff\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=3fc6cdcd4846d0c7d7b616139c1fd0c2180fc0ed8fe167d81e516ff22bcf28bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/ef/ae/073b491b14d25e2efafcffca9e16b2ee6d114ec5c643ba4f06\n",
            "Successfully built python-gflags rouge-score sqlitedict word2number\n",
            "Failed to build fast_hadamard_transform\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (fast_hadamard_transform)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import torch; print(torch.__path__)\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk7fGoiNreZ7",
        "outputId": "8faa08c5-9a30-4e31-f106-2e35c8431bf5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/usr/local/lib/python3.11/dist-packages/torch']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -I/usr/local/lib/python3.11/dist-packages/torch/include -o inference inference.cu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVMYhZv3r2zL",
        "outputId": "93623191-a0c7-4132-9c0b-5c079d88c018"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01minference.cu(225)\u001b[0m: \u001b[01;35mwarning\u001b[0m #191-D: type qualifier is meaningless on cast type\n",
            "          load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx, laneId, reg_cs_next, reg_cs2_next);\n",
            "                          ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01minference.cu(266)\u001b[0m: \u001b[01;35mwarning\u001b[0m #191-D: type qualifier is meaningless on cast type\n",
            "              load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx + (1 - ki % 2) * u16_per_tile_block, laneId, reg_cs_next, reg_cs2_next);\n",
            "                              ^\n",
            "\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/barrier:24\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/pipeline:66\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinference.cu:8\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/std/barrier:15:4:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K#error \"CUDA synchronization primitives are only supported for sm_70 and up.\"\n",
            "   15 | #  \u001b[01;31m\u001b[Kerror\u001b[m\u001b[K \"CUDA synchronization primitives are only supported for sm_70 and up.\"\n",
            "      |    \u001b[01;31m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda-12.5/targets/x86_64-linux/include/cuda/std/detail/libcxx/include/atomic:734\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda-12.5/targets/x86_64-linux/include/cuda/std/detail/libcxx/include/barrier:71\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/std/barrier:22\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/barrier:24\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/pipeline:66\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinference.cu:8\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda-12.5/targets/x86_64-linux/include/cuda/std/detail/libcxx/include/support/atomic/atomic_cuda.h:12:4:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K#error \"CUDA atomics are only supported for sm_60 and up on *nix and sm_70 and up on Windows.\"\n",
            "   12 | #  \u001b[01;31m\u001b[Kerror\u001b[m\u001b[K \"CUDA atomics are only supported for sm_60 and up on *nix and sm_70 and up on Windows.\"\n",
            "      |    \u001b[01;31m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda-12.5/targets/x86_64-linux/include/cuda/std/detail/libcxx/include/barrier:487\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/std/barrier:22\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/barrier:24\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/pipeline:66\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinference.cu:8\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda-12.5/targets/x86_64-linux/include/cuda/std/detail/libcxx/include/__cuda/barrier.h:19:4:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K#error \"CUDA synchronization primitives are only supported for sm_70 and up.\"\n",
            "   19 | #  \u001b[01;31m\u001b[Kerror\u001b[m\u001b[K \"CUDA synchronization primitives are only supported for sm_70 and up.\"\n",
            "      |    \u001b[01;31m\u001b[K^~~~~\u001b[m\u001b[K\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -I/usr/local/lib/python3.11/dist-packages/torch/include \\\n",
        "      -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include \\\n",
        "      -o inference inference.cu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpmjU5SdsQIU",
        "outputId": "71c80258-ffaf-485a-e3b6-cc7e7df80f50"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01minference.cu(225)\u001b[0m: \u001b[01;35mwarning\u001b[0m #191-D: type qualifier is meaningless on cast type\n",
            "          load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx, laneId, reg_cs_next, reg_cs2_next);\n",
            "                          ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01minference.cu(266)\u001b[0m: \u001b[01;35mwarning\u001b[0m #191-D: type qualifier is meaningless on cast type\n",
            "              load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx + (1 - ki % 2) * u16_per_tile_block, laneId, reg_cs_next, reg_cs2_next);\n",
            "                              ^\n",
            "\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/barrier:24\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/pipeline:66\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinference.cu:8\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/std/barrier:15:4:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K#error \"CUDA synchronization primitives are only supported for sm_70 and up.\"\n",
            "   15 | #  \u001b[01;31m\u001b[Kerror\u001b[m\u001b[K \"CUDA synchronization primitives are only supported for sm_70 and up.\"\n",
            "      |    \u001b[01;31m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda-12.5/targets/x86_64-linux/include/cuda/std/detail/libcxx/include/atomic:734\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda-12.5/targets/x86_64-linux/include/cuda/std/detail/libcxx/include/barrier:71\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/std/barrier:22\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/barrier:24\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/pipeline:66\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinference.cu:8\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda-12.5/targets/x86_64-linux/include/cuda/std/detail/libcxx/include/support/atomic/atomic_cuda.h:12:4:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K#error \"CUDA atomics are only supported for sm_60 and up on *nix and sm_70 and up on Windows.\"\n",
            "   12 | #  \u001b[01;31m\u001b[Kerror\u001b[m\u001b[K \"CUDA atomics are only supported for sm_60 and up on *nix and sm_70 and up on Windows.\"\n",
            "      |    \u001b[01;31m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/cuda-12.5/targets/x86_64-linux/include/cuda/std/detail/libcxx/include/barrier:487\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/std/barrier:22\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/barrier:24\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/cuda/bin/../targets/x86_64-linux/include/cuda/pipeline:66\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kinference.cu:8\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/cuda-12.5/targets/x86_64-linux/include/cuda/std/detail/libcxx/include/__cuda/barrier.h:19:4:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K#error \"CUDA synchronization primitives are only supported for sm_70 and up.\"\n",
            "   19 | #  \u001b[01;31m\u001b[Kerror\u001b[m\u001b[K \"CUDA synchronization primitives are only supported for sm_70 and up.\"\n",
            "      |    \u001b[01;31m\u001b[K^~~~~\u001b[m\u001b[K\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_70 -I/usr/local/lib/python3.11/dist-packages/torch/include -o inference inference.cu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vzwi9XQksQij",
        "outputId": "46149064-0191-44be-a1ed-4cfc63f473b0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01minference.cu(225)\u001b[0m: \u001b[01;35mwarning\u001b[0m #191-D: type qualifier is meaningless on cast type\n",
            "          load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx, laneId, reg_cs_next, reg_cs2_next);\n",
            "                          ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01minference.cu(266)\u001b[0m: \u001b[01;35mwarning\u001b[0m #191-D: type qualifier is meaningless on cast type\n",
            "              load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx + (1 - ki % 2) * u16_per_tile_block, laneId, reg_cs_next, reg_cs2_next);\n",
            "                              ^\n",
            "\n",
            "/usr/bin/ld: /usr/lib/gcc/x86_64-linux-gnu/11/../../../x86_64-linux-gnu/Scrt1.o: in function `_start':\n",
            "(.text+0x1b): undefined reference to `main'\n",
            "collect2: error: ld returned 1 exit status\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -I/usr/local/lib/python3.11/dist-packages/torch/include -o inference inference.cu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRD6Iw13sZQT",
        "outputId": "e8670cf8-0bd1-4a1c-e9f3-d72ff34baa64"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01minference.cu(225)\u001b[0m: \u001b[01;35mwarning\u001b[0m #191-D: type qualifier is meaningless on cast type\n",
            "          load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx, laneId, reg_cs_next, reg_cs2_next);\n",
            "                          ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01minference.cu(266)\u001b[0m: \u001b[01;35mwarning\u001b[0m #191-D: type qualifier is meaningless on cast type\n",
            "              load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx + (1 - ki % 2) * u16_per_tile_block, laneId, reg_cs_next, reg_cs2_next);\n",
            "                              ^\n",
            "\n",
            "/usr/bin/ld: /usr/lib/gcc/x86_64-linux-gnu/11/../../../x86_64-linux-gnu/Scrt1.o: in function `_start':\n",
            "(.text+0x1b): undefined reference to `main'\n",
            "collect2: error: ld returned 1 exit status\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hh-uBA8Hsh8j",
        "outputId": "12a338be-6266-4acc-911a-ff95cbb4ef62"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Cornell-RelaxML/qtip.git\n",
        "!cd qtip/qtip-kernels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mN39qgp9tgzU",
        "outputId": "34cccb0a-fb97-49ab-ce67-4e26043bf903"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'qtip'...\n",
            "remote: Enumerating objects: 612, done.\u001b[K\n",
            "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 612 (delta 29), reused 27 (delta 25), pack-reused 554 (from 1)\u001b[K\n",
            "Receiving objects: 100% (612/612), 884.50 KiB | 11.79 MiB/s, done.\n",
            "Resolving deltas: 100% (363/363), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/qtip/qtip-kernels/src"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5bpht1qt7O1",
        "outputId": "9f7e2a50-905c-4b2b-e329-f192d83150a4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qtip/qtip-kernels/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o inference.out src/inference.cu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXDWHDKFtq9L",
        "outputId": "45580602-92ee-43c1-c0d2-a3fd833e69b3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Ksrc/inference.cu: No such file or directory\n",
            "compilation terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o inference.out src/inference.cu -lcudart -rdc=true\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFqn1rlPtuxz",
        "outputId": "fab9f7c4-6e3c-4563-f06a-0a45f391198e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Ksrc/inference.cu: No such file or directory\n",
            "compilation terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o inference.out src/inference.cu -lcudart -rdc=true\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_W3Krkgt4G7",
        "outputId": "dcecdd8c-bba8-4d86-b55e-31e7282bd68a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Ksrc/inference.cu: No such file or directory\n",
            "compilation terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -c src/inference.cu -o inference.o\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT6jCCqiuCFT",
        "outputId": "c7158afe-0bbf-4f47-82f1-0fb01fc97b2e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Ksrc/inference.cu: No such file or directory\n",
            "compilation terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./inference.out\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE13RyKpuGtL",
        "outputId": "a01aa1b4-b666-4cae-f6e8-8ce28c005441"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: ./inference.out: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd qtip/qtip-kernels && ls src/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOgPJ0DeuI_L",
        "outputId": "bab0048b-c6b0-47b0-fcc2-58e63b655038"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "baseline.py  inference.cu  inference.h\tMakefile  qtip_torch.cu  test.cu  wrapper.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o inference.out src/inference.cu -lcudart -rdc=true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dEwpt--uQNj",
        "outputId": "1ffbb4aa-162d-4ca4-f00a-ec9bc6dddec2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Ksrc/inference.cu: No such file or directory\n",
            "compilation terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o inference.out /content/qtip/qtip-kernels/src/inference.cu -lcudart -rdc=true\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlBbah_huWWM",
        "outputId": "d366602d-d89c-495d-c98d-0ac705e20a41"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[K/content/qtip/qtip-kernels/src/inference.cu:11:10:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kc10/cuda/CUDAStream.h: No such file or directory\n",
            "   11 | #include \u001b[01;31m\u001b[K<c10/cuda/CUDAStream.h>\u001b[m\u001b[K\n",
            "      |          \u001b[01;31m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "compilation terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls -l\n",
        "!nvcc --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUAfwHlCubRz",
        "outputId": "682b640e-3980-4a92-8979-1f3ca66fe13f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qtip/qtip-kernels/src\n",
            "total 84\n",
            "-rw-r--r--  1 root root   497 Mar 14 03:19 baseline.py\n",
            "-rw-r--r--  1 root root 19479 Mar 14 03:29 inference.cu\n",
            "-rw-r--r--  1 root root  1269 Mar 14 03:32 inference.h\n",
            "-rw-r--r--  1 root root   432 Mar 14 03:19 Makefile\n",
            "drwxr-xr-x 10 root root  4096 Mar 14 03:33 qtip\n",
            "-rw-r--r--  1 root root 19596 Mar 14 03:19 qtip_torch.cu\n",
            "-rw-r--r--  1 root root  1955 Mar 14 03:19 test.cu\n",
            "-rw-r--r--  1 root root 20645 Mar 14 03:19 wrapper.cpp\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /usr -name \"CUDAStream.h\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQWAk3-AukZr",
        "outputId": "62227dc6-a94a-4c26-e32a-f91f4c8fae38"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/include/c10/cuda/CUDAStream.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import torch; print(torch.__path__)\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OL-53cryut5j",
        "outputId": "f1299d71-12bd-4b5f-f9a0-b5de1e8e1613"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/usr/local/lib/python3.11/dist-packages/torch']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o inference.out inference.cu -I/usr/local/lib/python3.10/dist-packages/torch/include -lcudart -rdc=true\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB2EBPl3u4wL",
        "outputId": "a887d642-258a-4610-fbf0-9a9bdc2aa6f1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[Kinference.cu:11:10:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kc10/cuda/CUDAStream.h: No such file or directory\n",
            "   11 | #include \u001b[01;31m\u001b[K<c10/cuda/CUDAStream.h>\u001b[m\u001b[K\n",
            "      |          \u001b[01;31m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "compilation terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env CKPT=/content/llama_quantized\n",
        "%env HF=/content/llama_hf\n",
        "%env LOG=/content/llama_logs\n",
        "%env HESS=/content/hessian_data\n",
        "%env CKPT=/content/llama_quantized\n",
        "%env HF=/content/llama_hf\n",
        "%env LOG=/content/llama_logs\n",
        "%env HESS=/content/hessian_data\n",
        "\n",
        "!mkdir -p $CKPT $HF $LOG\n",
        "!export HESS=/content/hessian_data\n",
        "\n",
        "!mkdir -p $CKPT $HF $LOG\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YA3Xrqa3vJy8",
        "outputId": "8a556248-70d1-4c50-dbd5-333f2c324d02"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CKPT=/content/llama_quantized\n",
            "env: HF=/content/llama_hf\n",
            "env: LOG=/content/llama_logs\n",
            "env: HESS=/content/hessian_data\n",
            "env: CKPT=/content/llama_quantized\n",
            "env: HF=/content/llama_hf\n",
            "env: LOG=/content/llama_logs\n",
            "env: HESS=/content/hessian_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m quantize_llama.quantize_finetune_llama \\\n",
        "       --save_path $CKPT/2_7b_2bit \\\n",
        "       --codebook bitshift \\\n",
        "       --base_model meta-llama/Llama-2-7b-hf \\\n",
        "       --in_hess_path $HESS \\\n",
        "       --scale_override 0.9 \\\n",
        "       --ft_epochs 5 \\\n",
        "       --td_x 16 \\\n",
        "       --td_y 16 \\\n",
        "       --L 16 \\\n",
        "       --K 2 \\\n",
        "       --V 2 \\\n",
        "       --decode_mode quantlut_sym \\\n",
        "       --tlut_bits 9 \\\n",
        "       >> $LOG/2_7b_2bit 2>&1\n"
      ],
      "metadata": {
        "id": "BCBgTKfPvfH0"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m quantize_llama.hfize_llama \\\n",
        "       --quantized_path $CKPT/2_7b_2bit \\\n",
        "       --hf_output_path $HF/2_7b_2bit \\\n",
        "       >> $LOG/2_7b_2bit 2>&1\n"
      ],
      "metadata": {
        "id": "yP9QO26Lvlc0"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m quantize_llama.finetune_e2e_llama \\\n",
        "       --base_model meta-llama/Llama-2-7b-hf \\\n",
        "       --hf_path $HF/2_7b_2bit \\\n",
        "       --devset_size 640 \\\n",
        "       --ft_valid_size 128 \\\n",
        "       --ft_epochs 4 \\\n",
        "       --ft_update_freq 4 \\\n",
        "       --ft_bs 2 \\\n",
        "       --ctx_size 4096 \\\n",
        "       --ft_train_lut \\\n",
        "       --hf_output_path $HF/2_7b_2bit \\\n",
        "       >> $LOG/2_7b_2bit 2>&1\n"
      ],
      "metadata": {
        "id": "2U4ZemjZvp3s"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m eval.eval_ppl  --hf_path $HF/2_7b_2bit >> $LOG/2_7b_2bit 2>&1\n",
        "\n"
      ],
      "metadata": {
        "id": "1s_Fv4oIvtvE"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m eval.eval_zeroshot \\\n",
        "       --tasks arc_challenge,arc_easy,boolq,piqa,winogrande \\\n",
        "       --batch_size 16 \\\n",
        "       --hf_path $HF/2_7b_2bit \\\n",
        "       >> $LOG/2_7b_2bit 2>&1\n"
      ],
      "metadata": {
        "id": "QnbWmkg9vv8E"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!export PYTHONPATH=$PYTHONPATH:/content/qtip\n"
      ],
      "metadata": {
        "id": "XYkab2wYvz7U"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# **حدد المسار إلى النموذج المضغوط**\n",
        "model_path = \"relaxml/Llama-2-7b-chat-QTIP-2Bit\"  # استبدلها بمسارك الفعلي\n",
        "\n",
        "# **تحميل التوكنيزر والنموذج**\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "# **اختبار النموذج**\n",
        "input_text = \"مرحبا! كيف يمكنني مساعدتك اليوم؟\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "# **توليد النص**\n",
        "output = model.generate(input_ids, max_length=100)\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"📝 النص المولّد:\", generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669,
          "referenced_widgets": [
            "a64eab1f5d844025829fd73d3b4ef713",
            "237775f85157454d8a67e1d8b52debad",
            "4bc2e34107854bf7b810a97c284605a9",
            "4aa01775d4054302968daad5c2b1ee6a",
            "e6ea9f05581f47779c0d45a2fe50a63e",
            "aad9af6d1c8d41719d6a6897156cd384",
            "3ae35104cef64397bb799ccd96865ecd",
            "839b20dddac249e8a6adfaee40ed0bc0",
            "6e050ef8144e4921921a8c36f5e83bf7",
            "7d9f01cff54b493abdeebf5ff429e624",
            "60f7e36bd24f4bddb7ed95590db1a6d5"
          ]
        },
        "id": "VKlel70zv26E",
        "outputId": "94ae8e4e-447f-4f2e-df28-fecb66c77e87"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/912 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a64eab1f5d844025829fd73d3b4ef713"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Can't load tokenizer for 'relaxml/Llama-2-7b-chat-QTIP-2Bit'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'relaxml/Llama-2-7b-chat-QTIP-2Bit' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-df3dbdded049>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# **تحميل التوكنيزر والنموذج**\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    954\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2018\u001b[0m         \u001b[0;31m# loaded directly from the GGUF file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2019\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_file_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull_file_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgguf_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2020\u001b[0;31m             raise EnvironmentError(\n\u001b[0m\u001b[1;32m   2021\u001b[0m                 \u001b[0;34mf\"Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m                 \u001b[0;34m\"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'relaxml/Llama-2-7b-chat-QTIP-2Bit'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'relaxml/Llama-2-7b-chat-QTIP-2Bit' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token XXXXXXXXXXX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Fs5Sk0qzzN2",
        "outputId": "c1e2196c-85f0-4198-a637-cb548338b28b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `read` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `read`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"relaxml/Llama-2-7b-QTIP-2Bit\")\n",
        "model = model.to(\"cpu\")\n",
        "# Input text\n",
        "input_text = \"Once upon a time, in a land far, far away, there was a\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate text\n",
        "output = model.generate(**inputs, max_length=100)\n",
        "\n",
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fk_8yCJdw7Sk",
        "outputId": "b31e3a67-cefe-4cf5-fe9f-8821c92211f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"relaxml/Llama-2-7b-QTIP-2Bit\")\n",
        "model = model.to(\"cuda\")\n",
        "# Input text\n",
        "input_text = \"Once upon a time, in a land far, far away, there was a\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate text\n",
        "output = model.generate(**inputs, max_length=5)\n",
        "\n",
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ywJ3r6Czt3N",
        "outputId": "cb533ead-f131-42fd-8779-f914313c808f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"relaxml/Llama-2-7b-QTIP-2Bit\", device_map=\"auto\")  # Use device_map=\"auto\"\n",
        "\n",
        "# Input text\n",
        "input_text = \"Once upon a time, in a land far, far away, there was a\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device) # Move inputs to the model's device\n",
        "\n",
        "# Generate text\n",
        "output = model.generate(**inputs, max_length=5)\n",
        "\n",
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(generated_text)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRl6Y9v61FUv",
        "outputId": "cfa66845-b373-4dba-d1de-5544a339d762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import torch\n",
        "from numba import cuda\n",
        "\n",
        "# التحقق من توفر CUDA\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA متوفر.\")\n",
        "\n",
        "# تعريف نواة CUDA\n",
        "@cuda.jit\n",
        "def my_kernel(x, y, out):\n",
        "    # كود النواة مستوحى من الكود السابق\n",
        "    ...\n",
        "\n",
        "# تهيئة البيانات\n",
        "x = torch.randn(1024, device='cuda')\n",
        "y = torch.randn(1024, device='cuda')\n",
        "out = torch.zeros(1024, device='cuda')\n",
        "\n",
        "# تحديد حجم الشبكة والكتلة\n",
        "threadsperblock = 32\n",
        "blockspergrid = (x.size(0) + (threadsperblock - 1)) // threadsperblock\n",
        "\n",
        "# إطلاق نواة CUDA\n",
        "my_kernel[blockspergrid, threadsperblock](x, y, out)\n",
        "\n",
        "# طباعة النتائج\n",
        "print(out)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "6A4-sF6H1-GH",
        "outputId": "c5c93f58-8963-43fc-cb08-cd2d85e23580"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA متوفر.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/dispatcher.py:579: NumbaPerformanceWarning: Grid size 32 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "ERROR:numba.cuda.cudadrv.driver:Call to cuLinkAddData results in CUDA_ERROR_UNSUPPORTED_PTX_VERSION\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LinkerError",
          "evalue": "[222] Call to cuLinkAddData results in CUDA_ERROR_UNSUPPORTED_PTX_VERSION\nptxas application ptx input, line 9; fatal   : Unsupported .version 8.5; current version is '8.4'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCudaAPIError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36madd_ptx\u001b[0;34m(self, ptx, name)\u001b[0m\n\u001b[1;32m   2919\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2920\u001b[0;31m             driver.cuLinkAddData(self.handle, enums.CU_JIT_INPUT_PTX,\n\u001b[0m\u001b[1;32m   2921\u001b[0m                                  ptxbuf, len(ptx), namebuf, 0, None, None)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36msafe_cuda_api_call\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_ctypes_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36m_check_ctypes_error\u001b[0;34m(self, fname, retcode)\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_fork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCudaAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCudaAPIError\u001b[0m: [222] Call to cuLinkAddData results in CUDA_ERROR_UNSUPPORTED_PTX_VERSION",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLinkerError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b97c76bcf8b1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# إطلاق نواة CUDA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmy_kernel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblockspergrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreadsperblock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# طباعة النتائج\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/dispatcher.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         return self.dispatcher.call(args, self.griddim, self.blockdim,\n\u001b[0m\u001b[1;32m    583\u001b[0m                                     self.stream, self.sharedmem)\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/dispatcher.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, args, griddim, blockdim, stream, sharedmem)\u001b[0m\n\u001b[1;32m    722\u001b[0m             \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverloads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m             \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgriddim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblockdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharedmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/dispatcher.py\u001b[0m in \u001b[0;36m_compile_for_args\u001b[0;34m(self, *args, **kws)\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkws\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0margtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypeof_pyval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtypeof_pyval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/dispatcher.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, sig)\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargetoptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m             \u001b[0;31m# We call bind to force codegen, so that there is a cubin to cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_overload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/dispatcher.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mForce\u001b[0m \u001b[0mbinding\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mCUDA\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \"\"\"\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_codelibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/codegen.py\u001b[0m in \u001b[0;36mget_cufunc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcufunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mcubin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cubin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_capability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_module_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcubin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/codegen.py\u001b[0m in \u001b[0;36mget_cubin\u001b[0;34m(self, cc)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0mlto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         )\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_link_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_nonlto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0mcubin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/codegen.py\u001b[0m in \u001b[0;36m_link_all\u001b[0;34m(self, linker, cc, ignore_nonlto)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mptx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_asm_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mlinker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_ptx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mptx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_linking_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36madd_ptx\u001b[0;34m(self, ptx, name)\u001b[0m\n\u001b[1;32m   2921\u001b[0m                                  ptxbuf, len(ptx), namebuf, 0, None, None)\n\u001b[1;32m   2922\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCudaAPIError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2923\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mLinkerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2925\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLinkerError\u001b[0m: [222] Call to cuLinkAddData results in CUDA_ERROR_UNSUPPORTED_PTX_VERSION\nptxas application ptx input, line 9; fatal   : Unsupported .version 8.5; current version is '8.4'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numba==0.56.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRiUYV682JRH",
        "outputId": "61a6f0a1-794b-48b1-e824-7e5a8026df4f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numba==0.56.4\n",
            "  Downloading numba-0.56.4.tar.gz (2.4 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/2.4 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TP2518GI2Jon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!apt-get --purge remove cuda nvidia* libnvinfer* libnvonnx*\n",
        "        !wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin\n",
        "        !mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
        "        !wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda-repo-ubuntu2004-11-8-local_11.8.0-520.61.07-1_amd64.deb\n",
        "        !dpkg -i cuda-repo-ubuntu2004-11-8-local_11.8.0-520.61.07-1_amd64.deb\n",
        "        !apt-key add /var/cuda-repo-ubuntu2004-11-8-local/7fa2af80.pub\n",
        "        !apt-get update\n",
        "        !apt-get -y install cuda"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "a_bcT_yR2Krp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/qtip\n",
        "from lib.utils.unsafe_import import model_from_hf_path\n",
        "import torch\n",
        "\n",
        "def get_quantized_layer(path2qmodel=\"relaxml/Llama-2-7b-QTIP-2Bit\"):\n",
        "    # load a quantized model\n",
        "    quant_model = model_from_hf_path(path2qmodel)[0].float()\n",
        "    # select an arbitrary layer.\n",
        "    quantized_layer = quant_model.model.layers[0].self_attn.q_proj\n",
        "\n",
        "    # replicate the routine in finetune_e2e_llama.py #L95:107 with --ft_train_lut flag\n",
        "    quantized_layer.SU = torch.nn.Parameter(quantized_layer.SU.float(), requires_grad=True)\n",
        "    quantized_layer.SV = torch.nn.Parameter(quantized_layer.SV.float(), requires_grad=True)\n",
        "    quantized_layer.mode = \"train-recons\"\n",
        "    quantized_layer.tlut.requires_grad = True\n",
        "\n",
        "    return quantized_layer\n",
        "\n",
        "def reinit_grad(quantized_layer):\n",
        "    quantized_layer.SU.grad = None\n",
        "    quantized_layer.SV.grad = None\n",
        "    quantized_layer.tlut.grad = None\n",
        "\n",
        "def test_backward():\n",
        "    # load quantized layer\n",
        "    quantized_layer = get_quantized_layer()\n",
        "\n",
        "    # initialize random input to the layer\n",
        "    ft_bs, ctx_size, in_features = 4, 4096, 4096\n",
        "    input = torch.randn(ft_bs, ctx_size, in_features).to('cuda').to(torch.float16)\n",
        "    input.requires_grad = True\n",
        "\n",
        "    # prev forward pass\n",
        "    quantized_layer.use_prev_kernel = True # flag for new forward pass\n",
        "    output = quantized_layer(input)\n",
        "\n",
        "    # backward pass\n",
        "    print(\"=== Prev kernel ===\")\n",
        "    loss = output.sum()\n",
        "    loss.backward()\n",
        "\n",
        "    print(\"loss\", loss.item())\n",
        "    print(\"input grad\", input.grad.shape)\n",
        "    print(\"SU grad\", quantized_layer.SU.grad.shape)\n",
        "    print(\"SV grad\", quantized_layer.SV.grad.shape)\n",
        "    print(\"tlut grad\", quantized_layer.tlut.grad)\n",
        "\n",
        "    prev_input_grad, prev_SU_grad, prev_SV_grad = input.grad.clone(), quantized_layer.SU.grad.clone(), quantized_layer.SV.grad.clone()\n",
        "    prev_loss = loss.clone()\n",
        "    reinit_grad(quantized_layer)\n",
        "    input.grad = None\n",
        "\n",
        "    # new forward pass\n",
        "    quantized_layer.use_prev_kernel = False # flag for new forward pass\n",
        "    output = quantized_layer(input)\n",
        "\n",
        "    # backward pass\n",
        "    print(\"=== New kernel ===\")\n",
        "    loss = output.sum()\n",
        "    loss.backward()\n",
        "\n",
        "    print(\"loss\", loss.item())\n",
        "    if torch.abs(prev_loss.item() - loss.item()) <= 1e-8:\n",
        "        print(\"\\tEqual loss\")\n",
        "    print(\"input grad\", input.grad.shape)\n",
        "    if torch.allclose(prev_input_grad, input.grad):\n",
        "        print(\"\\tEqual input grad\")\n",
        "    print(\"SU grad\", quantized_layer.SU.grad.shape)\n",
        "    if torch.allclose(prev_SU_grad, quantized_layer.SU.grad):\n",
        "        print(\"\\tEqual SU grad\")\n",
        "    print(\"SV grad\", quantized_layer.SV.grad.shape)\n",
        "    if torch.allclose(prev_SV_grad, quantized_layer.SV.grad):\n",
        "        print(\"\\tEqual SV grad\")\n",
        "    print(\"tlut grad\", quantized_layer.tlut.grad)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_backward()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "rOeiwZCZ2idW",
        "outputId": "f2ee1eec-e3dc-4513-a772-05666e18f488"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qtip\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'lib.utils'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d1da6b0495d2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/qtip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsafe_import\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_from_hf_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_quantized_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath2qmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relaxml/Llama-2-7b-QTIP-2Bit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lib.utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RMfYfBp82nWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install llvmlite==0.39.1"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RPP9XkT25Xn",
        "outputId": "170dd3f5-930f-4534-93ad-c22bc29f8a3f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llvmlite==0.39.1\n",
            "  Downloading llvmlite-0.39.1.tar.gz (132 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/132.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install numba==0.56.4"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCaftG0m25p3",
        "outputId": "d7cc9475-23f5-4eab-87a6-07e90cdee852"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numba==0.56.4\n",
            "  Using cached numba-0.56.4.tar.gz (2.4 MB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/qtip/qtip-kernels/src\n",
        "!make"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSjx8zrf3b-3",
        "outputId": "e3724014-455b-4223-ea73-69b11d69941a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qtip/qtip-kernels/src\n",
            "nvcc \\\n",
            "\t-O3 -lineinfo -Xcompiler -rdynamic --use_fast_math -keep -std=c++17 --ptxas-options=-v \\\n",
            "\t-gencode arch=compute_86,code=sm_86  \\\n",
            "\t-gencode arch=compute_89,code=sm_89  \\\n",
            "\t-gencode arch=compute_90,code=sm_90  \\\n",
            "\t-o test test.cu inference.cu\n",
            "In file included from \u001b[01m\u001b[Ktest.cu:5\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kinference.cu:11:10:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kc10/cuda/CUDAStream.h: No such file or directory\n",
            "   11 | #include \u001b[01;31m\u001b[K<c10/cuda/CUDAStream.h>\u001b[m\u001b[K\n",
            "      |          \u001b[01;31m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "compilation terminated.\n",
            "make: *** [Makefile:9: test] Error 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/qtip/qtip-kernels/src\n",
        "!cmake"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y72FjSbu3iYu",
        "outputId": "df0ea080-3fb8-4eee-e5c5-1297587e21d0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qtip/qtip-kernels/src\n",
            "Usage\n",
            "\n",
            "  cmake [options] <path-to-source>\n",
            "  cmake [options] <path-to-existing-build>\n",
            "  cmake [options] -S <path-to-source> -B <path-to-build>\n",
            "\n",
            "Specify a source directory to (re-)generate a build system for it in the\n",
            "current working directory.  Specify an existing build directory to\n",
            "re-generate its build system.\n",
            "\n",
            "Run 'cmake --help' for more information.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/qtip/qtip-kernels/src\n",
        "!cmake"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df0ea080-3fb8-4eee-e5c5-1297587e21d0",
        "id": "9Q5bG4Yu3sUC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qtip/qtip-kernels/src\n",
            "Usage\n",
            "\n",
            "  cmake [options] <path-to-source>\n",
            "  cmake [options] <path-to-existing-build>\n",
            "  cmake [options] -S <path-to-source> -B <path-to-build>\n",
            "\n",
            "Specify a source directory to (re-)generate a build system for it in the\n",
            "current working directory.  Specify an existing build directory to\n",
            "re-generate its build system.\n",
            "\n",
            "Run 'cmake --help' for more information.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "source": [
        "%cd /content/qtip/qtip-kernels/src\n",
        "!mkdir build\n",
        "%cd build"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncadd0OQ33Jm",
        "outputId": "6668c8bc-b505-4664-9617-7dc969d1ee31"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qtip/qtip-kernels/src\n",
            "/content/qtip/qtip-kernels/src/build\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!cmake .."
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnwbzyG-35L_",
        "outputId": "5817b318-d8ea-4eca-c316-3d8974b80f00"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mCMake Warning:\n",
            "  Ignoring extra path from command line:\n",
            "\n",
            "   \"..\"\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Error: The source directory \"/content/qtip/qtip-kernels/src\" does not appear to contain CMakeLists.txt.\n",
            "Specify --help for usage, or press the help button on the CMake GUI.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "source": [
        "%cd /content/qtip/qtip-kernels\n",
        "!mkdir build\n",
        "%cd build\n",
        "!cmake ..\n",
        "!make"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-bnLdqF4BA3",
        "outputId": "2fb89603-f0b2-4740-d5e9-8b700ad7e940"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qtip/qtip-kernels\n",
            "/content/qtip/qtip-kernels/build\n",
            "\u001b[33mCMake Warning:\n",
            "  Ignoring extra path from command line:\n",
            "\n",
            "   \"..\"\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Error: The source directory \"/content/qtip/qtip-kernels\" does not appear to contain CMakeLists.txt.\n",
            "Specify --help for usage, or press the help button on the CMake GUI.\u001b[0m\n",
            "make: *** No targets specified and no makefile found.  Stop.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def benchmark():\n",
        "    m, n, k = 8192, 8, 8192\n",
        "    print(f\"{m = }, {n = }, {k = }\")\n",
        "\n",
        "    decompressed = torch.randn((m, k), dtype=torch.float16,\n",
        "                               device=\"cpu\").cuda()\n",
        "    x = torch.randn((n, k), dtype=torch.float16, device=\"cpu\").cuda()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    result = decompressed @ x.T\n",
        "    dummy = torch.sum(result)\n",
        "    print(dummy.item())\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(42)\n",
        "    benchmark()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isYl8hCe3n3O",
        "outputId": "adb97782-245c-468b-8d77-7712b89ab3f5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "m = 8192, n = 8, k = 8192\n",
            "15168.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f *.o *.ii *.cubin *.fatbin.c *.fatbin *.cudafe1.* *.module_id *.ptx *.reg.c test\n",
        "\n",
        "!test: test.cu inference.cu inference.h\n",
        "!nvcc \\\n",
        "\t\t-O3 -lineinfo -Xcompiler -rdynamic --use_fast_math -keep -std=c++17 --ptxas-options=-v \\\n",
        "\t\t-gencode arch=compute_86,code=sm_86  \\\n",
        "\t\t-gencode arch=compute_89,code=sm_89  \\\n",
        "\t  -gencode arch=compute_90,code=sm_90  \\\n",
        "\t\t-o $@ test.cu inference.cu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TagCEX_04PLn",
        "outputId": "ed9ce852-a2bf-4f5e-8e70-7ed455a28221"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: test:: command not found\n",
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kinference.cu: No such file or directory\n",
            "compilation terminated.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "%cd /content/qtip/qtip-kernels/src\n",
        "\n",
        "!cmake ../src\n",
        "!make"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e06OhVpf4si4",
        "outputId": "d54139ba-a921-405d-b752-0d439ac7bbce"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qtip/qtip-kernels/src\n",
            "\u001b[33mCMake Warning:\n",
            "  Ignoring extra path from command line:\n",
            "\n",
            "   \"../src\"\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Error: The source directory \"/content/qtip/qtip-kernels/src\" does not appear to contain CMakeLists.txt.\n",
            "Specify --help for usage, or press the help button on the CMake GUI.\u001b[0m\n",
            "nvcc \\\n",
            "\t-O3 -lineinfo -Xcompiler -rdynamic --use_fast_math -keep -std=c++17 --ptxas-options=-v \\\n",
            "\t-gencode arch=compute_86,code=sm_86  \\\n",
            "\t-gencode arch=compute_89,code=sm_89  \\\n",
            "\t-gencode arch=compute_90,code=sm_90  \\\n",
            "\t-o test test.cu inference.cu\n",
            "In file included from \u001b[01m\u001b[Ktest.cu:5\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kinference.cu:11:10:\u001b[m\u001b[K \u001b[01;31m\u001b[Kfatal error: \u001b[m\u001b[Kc10/cuda/CUDAStream.h: No such file or directory\n",
            "   11 | #include \u001b[01;31m\u001b[K<c10/cuda/CUDAStream.h>\u001b[m\u001b[K\n",
            "      |          \u001b[01;31m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "compilation terminated.\n",
            "make: *** [Makefile:9: test] Error 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/qtip\n",
        "!wget https://huggingface.co/ikawrakow/llama-v1-2bit-gguf/resolve/main/llama-v1-7b-q2k.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxH7fkpS4ZaH",
        "outputId": "3f38a5c8-cfef-46ba-de18-34ddc5c5607b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qtip\n",
            "--2025-03-14 04:29:56--  https://huggingface.co/ikawrakow/llama-v1-2bit-gguf/resolve/main/llama-v1-7b-q2k.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.160.59, 3.165.160.61, 3.165.160.12, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.160.59|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/1d/24/1d241edbf2aca5bd4a922873d3b144fcebdf7e7d9c5fa9365a5973e2b8cff23e/1a0c76ce735920f482ae67e02970fdbe745628a400350f4b642b7432db1703e8?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27llama-v1-7b-q2k.gguf%3B+filename%3D%22llama-v1-7b-q2k.gguf%22%3B&Expires=1741930196&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MTkzMDE5Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzFkLzI0LzFkMjQxZWRiZjJhY2E1YmQ0YTkyMjg3M2QzYjE0NGZjZWJkZjdlN2Q5YzVmYTkzNjVhNTk3M2UyYjhjZmYyM2UvMWEwYzc2Y2U3MzU5MjBmNDgyYWU2N2UwMjk3MGZkYmU3NDU2MjhhNDAwMzUwZjRiNjQyYjc0MzJkYjE3MDNlOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=YMQdEB7XPRAx66Eu0WKWHAAJ3-HDlYJS82PNeBTn85jFljTSQEhYFzlQdGvrqyCCtTi1gHnhFdyQHmdU4nxpbcv3qQ4JLzv%7ECDroUhLcKmuUm3fROqYTI1L1yeVluqfg44vTVQe%7ExDbyQBe%7EdFT1T69z71tYk3zMNIoBdOxflh%7Eg9-vyocMxlVArsfXO3qm6n53jKw%7EgWSMX44af1OXkBFf2DOv4JyfTJrKFur2h7s%7EewR-vMViogI%7Eyu0Xu7oXzsN%7ENOkbz0fE3%7E7H0ElsCFiRBS21vc8Fuzq404-IY5%7EbPqBPWm4UC6BOAmZ6zTnjdwSJuyxpXe2SJGOJ2jJbsVw__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-03-14 04:29:56--  https://cdn-lfs-us-1.hf.co/repos/1d/24/1d241edbf2aca5bd4a922873d3b144fcebdf7e7d9c5fa9365a5973e2b8cff23e/1a0c76ce735920f482ae67e02970fdbe745628a400350f4b642b7432db1703e8?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27llama-v1-7b-q2k.gguf%3B+filename%3D%22llama-v1-7b-q2k.gguf%22%3B&Expires=1741930196&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MTkzMDE5Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzFkLzI0LzFkMjQxZWRiZjJhY2E1YmQ0YTkyMjg3M2QzYjE0NGZjZWJkZjdlN2Q5YzVmYTkzNjVhNTk3M2UyYjhjZmYyM2UvMWEwYzc2Y2U3MzU5MjBmNDgyYWU2N2UwMjk3MGZkYmU3NDU2MjhhNDAwMzUwZjRiNjQyYjc0MzJkYjE3MDNlOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=YMQdEB7XPRAx66Eu0WKWHAAJ3-HDlYJS82PNeBTn85jFljTSQEhYFzlQdGvrqyCCtTi1gHnhFdyQHmdU4nxpbcv3qQ4JLzv%7ECDroUhLcKmuUm3fROqYTI1L1yeVluqfg44vTVQe%7ExDbyQBe%7EdFT1T69z71tYk3zMNIoBdOxflh%7Eg9-vyocMxlVArsfXO3qm6n53jKw%7EgWSMX44af1OXkBFf2DOv4JyfTJrKFur2h7s%7EewR-vMViogI%7Eyu0Xu7oXzsN%7ENOkbz0fE3%7E7H0ElsCFiRBS21vc8Fuzq404-IY5%7EbPqBPWm4UC6BOAmZ6zTnjdwSJuyxpXe2SJGOJ2jJbsVw__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.165.160.38, 3.165.160.20, 3.165.160.3, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.165.160.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2277273152 (2.1G) [application/octet-stream]\n",
            "Saving to: ‘llama-v1-7b-q2k.gguf’\n",
            "\n",
            "llama-v1-7b-q2k.ggu 100%[===================>]   2.12G  39.3MB/s    in 59s     \n",
            "\n",
            "2025-03-14 04:30:56 (36.9 MB/s) - ‘llama-v1-7b-q2k.gguf’ saved [2277273152/2277273152]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama.from_pretrained(\n",
        "\trepo_id=\"ikawrakow/llama-v1-2bit-gguf\",\n",
        "\tfilename=\"llama-v1-13b-q2k.gguf\",\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "GzSETbe_6n2o",
        "outputId": "0a108835-094b-4ab6-8187-87be0c2f429c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'llama_cpp'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-fdf9a596f296>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_cpp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLlama\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m llm = Llama.from_pretrained(\n\u001b[1;32m      4\u001b[0m         \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ikawrakow/llama-v1-2bit-gguf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama-v1-13b-q2k.gguf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "source": [
        "!pip install llama-cpp-python"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4WeDfae8U9J",
        "outputId": "364e20c1-4133-471d-f4de-6ef4a5ab029f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.8.tar.gz (67.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (1.26.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.8-cp311-cp311-linux_x86_64.whl size=5959586 sha256=927f36edb51c186207e79fdd833729e8479df142d8ed1ecde6eb01af9eaa36fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/03/66/eb3810eafd55d921b2be32896d1f44313996982360663aa80b\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/bin/bash\n",
        "\n",
        "LOG=/mnt/desa_data/logs\n",
        "\n",
        "CUDA_VISIBLE_DEVICES=0 python eval_ppl.py --model-dir /mnt/desa_data/q2k/llama-v1-7b-q2k.gguf --use_flash_attention_2 --n-gpu-layers 10000 >> $LOG/1_7b_q2k 2>&1 &\n",
        "CUDA_VISIBLE_DEVICES=1 python eval_ppl.py --model-dir /mnt/desa_data/q2k/llama-v1-13b-q2k.gguf --use_flash_attention_2 --n-gpu-layers 10000 >> $LOG/1_13b_q2k 2>&1 &\n",
        "CUDA_VISIBLE_DEVICES=2 python eval_ppl.py --model-dir /mnt/desa_data/q2k/llama-v1-30b-q2k.gguf --use_flash_attention_2 --n-gpu-layers 10000 >> $LOG/1_30b_q2k 2>&1 &\n",
        "CUDA_VISIBLE_DEVICES=3 python eval_ppl.py --model-dir /mnt/desa_data/q2k/llama-v1-65b-q2k.gguf --use_flash_attention_2 --n-gpu-layers 10000 >> $LOG/1_65b_q2k 2>&1 &\n",
        "CUDA_VISIBLE_DEVICES=4 python eval_ppl.py --model-dir /mnt/desa_data/q2k/llama-v2-7b-q2k.gguf --use_flash_attention_2 --n-gpu-layers 10000 >> $LOG/2_7b_q2k 2>&1 &\n",
        "CUDA_VISIBLE_DEVICES=5 python eval_ppl.py --model-dir /mnt/desa_data/q2k/llama-v2-13b-q2k.gguf --use_flash_attention_2 --n-gpu-layers 10000 >> $LOG/2_13b_q2k 2>&1 &\n",
        "CUDA_VISIBLE_DEVICES=6 python eval_ppl.py --model-dir /mnt/desa_data/q2k/llama-v2-70b-q2k.gguf --use_flash_attention_2 --n-gpu-layers 10000 >> $LOG/2_70b_q2k 2>&1 &\n",
        "\n",
        "wait"
      ],
      "metadata": {
        "id": "V5M0z1UY8VT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/content/qtip/llama-v1-7b-q2k.gguf"
      ],
      "metadata": {
        "id": "6PiDaPtx8lRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(\n",
        "      model_path=\"/content/qtip/llama-v1-7b-q2k.gguf\",\n",
        "      n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
        "      # seed=1337, # Uncomment to set a specific seed\n",
        "      # n_ctx=2048, # Uncomment to increase the context window\n",
        ")\n",
        "output = llm(\n",
        "      \"Q: Name the planets in the solar system? A: \", # Prompt\n",
        "      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
        "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
        "      echo=True # Echo the prompt back in the output\n",
        ") # Generate a completion, can also call create_completion\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbX3iTKj8wcy",
        "outputId": "fcb382ff-f098-4d3d-efb2-90ef1ebeca57"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 16 key-value pairs and 291 tensors from /content/qtip/llama-v1-7b-q2k.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q2_K:  225 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q2_K - Medium\n",
            "print_info: file size   = 2.12 GiB (2.70 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 3\n",
            "load: token to piece cache size = 0.1684 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 2048\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 32\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 4096\n",
            "print_info: n_embd_v_gqa     = 4096\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 11008\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 2048\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 6.74 B\n",
            "print_info: general.name     = LLaMA\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32000\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU\n",
            "load_tensors: layer   1 assigned to device CPU\n",
            "load_tensors: layer   2 assigned to device CPU\n",
            "load_tensors: layer   3 assigned to device CPU\n",
            "load_tensors: layer   4 assigned to device CPU\n",
            "load_tensors: layer   5 assigned to device CPU\n",
            "load_tensors: layer   6 assigned to device CPU\n",
            "load_tensors: layer   7 assigned to device CPU\n",
            "load_tensors: layer   8 assigned to device CPU\n",
            "load_tensors: layer   9 assigned to device CPU\n",
            "load_tensors: layer  10 assigned to device CPU\n",
            "load_tensors: layer  11 assigned to device CPU\n",
            "load_tensors: layer  12 assigned to device CPU\n",
            "load_tensors: layer  13 assigned to device CPU\n",
            "load_tensors: layer  14 assigned to device CPU\n",
            "load_tensors: layer  15 assigned to device CPU\n",
            "load_tensors: layer  16 assigned to device CPU\n",
            "load_tensors: layer  17 assigned to device CPU\n",
            "load_tensors: layer  18 assigned to device CPU\n",
            "load_tensors: layer  19 assigned to device CPU\n",
            "load_tensors: layer  20 assigned to device CPU\n",
            "load_tensors: layer  21 assigned to device CPU\n",
            "load_tensors: layer  22 assigned to device CPU\n",
            "load_tensors: layer  23 assigned to device CPU\n",
            "load_tensors: layer  24 assigned to device CPU\n",
            "load_tensors: layer  25 assigned to device CPU\n",
            "load_tensors: layer  26 assigned to device CPU\n",
            "load_tensors: layer  27 assigned to device CPU\n",
            "load_tensors: layer  28 assigned to device CPU\n",
            "load_tensors: layer  29 assigned to device CPU\n",
            "load_tensors: layer  30 assigned to device CPU\n",
            "load_tensors: layer  31 assigned to device CPU\n",
            "load_tensors: layer  32 assigned to device CPU\n",
            "load_tensors: tensor 'token_embd.weight' (q2_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =  2171.07 MiB\n",
            "................................................................................................\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 512\n",
            "llama_init_from_model: n_ctx_per_seq = 512\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 10000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (2048) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 28: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 29: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 30: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 31: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
            "llama_init_from_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =    70.50 MiB\n",
            "llama_init_from_model: graph nodes  = 1030\n",
            "llama_init_from_model: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'general.quantization_version': '2', 'general.file_type': '10', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'general.name': 'LLaMA', 'llama.block_count': '32', 'llama.context_length': '2048', 'general.architecture': 'llama'}\n",
            "Using fallback chat format: llama-2\n",
            "llama_perf_context_print:        load time =    6757.19 ms\n",
            "llama_perf_context_print: prompt eval time =    6757.02 ms /    15 tokens (  450.47 ms per token,     2.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =   10652.92 ms /    25 runs   (  426.12 ms per token,     2.35 tokens per second)\n",
            "llama_perf_context_print:       total time =   17423.27 ms /    40 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'cmpl-5287ef8a-2a01-4f61-8197-4cf9aa15d411', 'object': 'text_completion', 'created': 1741927327, 'model': '/content/qtip/llama-v1-7b-q2k.gguf', 'choices': [{'text': 'Q: Name the planets in the solar system? A: 6. Jupiter, Saturn, Neptune, Uranus, Mars, Venus, Mercury. ', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 15, 'completion_tokens': 26, 'total_tokens': 41}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/ikawrakow/llama-v1-2bit-gguf/resolve/main/llama-v1-70b-q2k.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9Zt3n4t9nIZ",
        "outputId": "e2e3ed9e-430f-4dd2-ec4e-1fc0a98d5b2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-14 04:43:04--  https://huggingface.co/ikawrakow/llama-v1-2bit-gguf/resolve/main/llama-v1-70b-q2k.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.160.12, 3.165.160.11, 3.165.160.61, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.160.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/1d/24/1d241edbf2aca5bd4a922873d3b144fcebdf7e7d9c5fa9365a5973e2b8cff23e/8e4d4e59200c6a6a6804281400784413045a3c52ad816070683f47fd8b78712a?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27llama-v1-70b-q2k.gguf%3B+filename%3D%22llama-v1-70b-q2k.gguf%22%3B&Expires=1741930985&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MTkzMDk4NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzFkLzI0LzFkMjQxZWRiZjJhY2E1YmQ0YTkyMjg3M2QzYjE0NGZjZWJkZjdlN2Q5YzVmYTkzNjVhNTk3M2UyYjhjZmYyM2UvOGU0ZDRlNTkyMDBjNmE2YTY4MDQyODE0MDA3ODQ0MTMwNDVhM2M1MmFkODE2MDcwNjgzZjQ3ZmQ4Yjc4NzEyYT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=VcJaqAQdimw2Go42tfBSgABfCPCRJToQcB8-4L1hAeiqJ2IZHNKBbDZduH1Oynhk4QQKgJon6p4HIBA3iY7Be0qACR7jh5m3Awv0y4MET4FrQe57l4tYI5Duoqf5YusPVwLSCnYCuAfGABEwCn7qmm1B%7EBc9D-Ru3r5d9VbyL5U240Jjd6gqd2mBUbovt7J8C65Ku7rWcA32mo3ntVTpvjCNsypz2VfWVBP8qsr%7EfHzp4J8gPzUPVSA8FH%7EHEN1nsSHfuKvYGQbiCiZv9ijVEe7073YObyAFY-F3KGOuwdK8H19NyUbJG1vDIYFQTiSjSyL-z8DEOdc2T6mL27PuMA__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-03-14 04:43:05--  https://cdn-lfs-us-1.hf.co/repos/1d/24/1d241edbf2aca5bd4a922873d3b144fcebdf7e7d9c5fa9365a5973e2b8cff23e/8e4d4e59200c6a6a6804281400784413045a3c52ad816070683f47fd8b78712a?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27llama-v1-70b-q2k.gguf%3B+filename%3D%22llama-v1-70b-q2k.gguf%22%3B&Expires=1741930985&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MTkzMDk4NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzFkLzI0LzFkMjQxZWRiZjJhY2E1YmQ0YTkyMjg3M2QzYjE0NGZjZWJkZjdlN2Q5YzVmYTkzNjVhNTk3M2UyYjhjZmYyM2UvOGU0ZDRlNTkyMDBjNmE2YTY4MDQyODE0MDA3ODQ0MTMwNDVhM2M1MmFkODE2MDcwNjgzZjQ3ZmQ4Yjc4NzEyYT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=VcJaqAQdimw2Go42tfBSgABfCPCRJToQcB8-4L1hAeiqJ2IZHNKBbDZduH1Oynhk4QQKgJon6p4HIBA3iY7Be0qACR7jh5m3Awv0y4MET4FrQe57l4tYI5Duoqf5YusPVwLSCnYCuAfGABEwCn7qmm1B%7EBc9D-Ru3r5d9VbyL5U240Jjd6gqd2mBUbovt7J8C65Ku7rWcA32mo3ntVTpvjCNsypz2VfWVBP8qsr%7EfHzp4J8gPzUPVSA8FH%7EHEN1nsSHfuKvYGQbiCiZv9ijVEe7073YObyAFY-F3KGOuwdK8H19NyUbJG1vDIYFQTiSjSyL-z8DEOdc2T6mL27PuMA__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 18.172.170.61, 18.172.170.37, 18.172.170.40, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.172.170.61|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21556491296 (20G) [binary/octet-stream]\n",
            "Saving to: ‘llama-v1-70b-q2k.gguf’\n",
            "\n",
            "llama-v1-70b-q2k.gg  18%[==>                 ]   3.81G  49.8MB/s    eta 6m 31s "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bz8ZPhF99oT4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}